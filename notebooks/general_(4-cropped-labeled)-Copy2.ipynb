{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage import transform # io, \n",
    "# import PIL\n",
    "# import math\n",
    "# from glob import glob\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms, utils, datasets, models\n",
    "# import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "# import torchvision\n",
    "# import pickle\n",
    "\n",
    "# # Ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# %matplotlib inline\n",
    "# plt.ion()   # interactive mode\n",
    "\n",
    "\n",
    "# from visualize import show_2Dbatch, show_4channel_batch, show_3Dbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# can revome the lines that need these\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datasets import GeneralDataset\n",
    "import Transforms as myTransforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 112\n"
     ]
    }
   ],
   "source": [
    "# choose task: either 'idh' or '1p19q'\n",
    "task = 'idh'\n",
    "\n",
    "# choose mtl: True or False\n",
    "mtl = False\n",
    "\n",
    "# choose MR input\n",
    "## details ('crop3Dslice', 'modality3D'\n",
    "dataformat = 'modality3D'\n",
    "\n",
    "# modality (either None, 'T1ce', 'FLAIR', 'T2', 'T1', 'T1ce-T1')\n",
    "modality = None\n",
    "\n",
    "# metadata for all brats (including tcia) data\n",
    "glioma_metadata_df = pd.read_csv('../data/glioma_metadata.csv', index_col=0)\n",
    "glioma_metadata_df.loc[['Brats18_TCIA09_462_1', 'Brats18_TCIA10_236_1'], 'idh'] = 1\n",
    "\n",
    "if task == 'idh':\n",
    "    classes = ['wildtype', 'mutant']\n",
    "    val_df = glioma_metadata_df.loc[(glioma_metadata_df['phase'] == 'val') & (glioma_metadata_df['idh'].isin([0,1]))]\n",
    "    if mtl:\n",
    "        train_df = glioma_metadata_df.loc[glioma_metadata_df['phase'].isin(['train', 'unlabeled train'])]\n",
    "    else:\n",
    "        train_df = glioma_metadata_df.loc[(glioma_metadata_df['phase'] == 'train') & (glioma_metadata_df['idh'].isin([0,1]))]\n",
    "        \n",
    "elif task == '1p19q':\n",
    "    classes = ['non-codel', 'oligo']\n",
    "    val_df = glioma_metadata_df.loc[glioma_metadata_df['phase'] == 'val']\n",
    "\n",
    "# map between brats dataset and tcia data (tcia data is avalible for a subset of the brats patients)\n",
    "# brats2tcia_df = pd.read_csv('../data/brats2tcia_df_542x1.csv', index_col=0)\n",
    "brats2tcia_df =glioma_metadata_df['tciaID']\n",
    "\n",
    "\n",
    "# these are labeled files (they were paths in old dataloader) but they are dataframes\n",
    "csv_files = {'train':train_df, 'val':val_df, 'data':glioma_metadata_df}\n",
    "\n",
    "\n",
    "genomic_csv_files = {'train':'../data/MGL/MGL_235x50.csv', \n",
    "                     'val':'../data/MGL/MGL_235x50.csv',\n",
    "                     'data':'../data/MGL/MGL_235x50.csv'}\n",
    "\n",
    "\n",
    "image_dir = '../data/all_brats_scans/'\n",
    "cluster_column = task\n",
    "\n",
    "train_batch_size = 4\n",
    "val_batch_size = 4\n",
    "data_batch_size = 1\n",
    "\n",
    "\n",
    "\n",
    "# resize_shape = (16, 16, 16)\n",
    "channels = 4\n",
    "interpolation = 1\n",
    "\n",
    "shuffle = True\n",
    "shuffle_data = False\n",
    "\n",
    "resize_shape = (64, 64, 64)\n",
    "print('Train size', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wildtype', 'mutant']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformations = myTransforms.Compose([\n",
    "        myTransforms.MinMaxNormalize(),\n",
    "        myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                  interpolation=interpolation, \n",
    "                                  channels=channels),\n",
    "        myTransforms.ZeroSprinkle(prob_zero=0.2, prob_true=0.8),\n",
    "        myTransforms.ZeroChannel(prob_zero=0.5),\n",
    "        myTransforms.RandomFlip(),\n",
    "        myTransforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "seg_transformations = myTransforms.Compose([\n",
    "    myTransforms.ScaleToFixed((1, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                  interpolation=0, \n",
    "                                  channels=1),\n",
    "        myTransforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "seg_probs_transformations = myTransforms.Compose([\n",
    "        myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                  interpolation=interpolation, \n",
    "                                  channels=channels),\n",
    "        myTransforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "val_transformations = myTransforms.Compose([\n",
    "        myTransforms.MinMaxNormalize(),\n",
    "        myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                  interpolation=interpolation, \n",
    "                                  channels=channels),\n",
    "#         myTransforms.RandomFlip(),\n",
    "        myTransforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "data_transforms = {'train': train_transformations,\n",
    "                   'val':   val_transformations,\n",
    "                   'data':   val_transformations,\n",
    "                   'seg': seg_transformations,\n",
    "                   'seg_probs':seg_probs_transformations\n",
    "                  }\n",
    "\n",
    "\n",
    "\n",
    "transformed_dataset_train = GeneralDataset(csv_file=csv_files['train'],\n",
    "                                           root_dir=image_dir,\n",
    "                                           genomic_csv_file = genomic_csv_files['train'],\n",
    "                                           transform=data_transforms['train'],\n",
    "                                           seg_transform=data_transforms['seg'],\n",
    "                                           label='cluster',\n",
    "                                           classes=classes,\n",
    "                                           dataformat=dataformat,\n",
    "                                           returndims=resize_shape,\n",
    "                                           return_max_slice=False,\n",
    "                                           brats2tcia_df=brats2tcia_df,\n",
    "                                           modality=None)\n",
    "\n",
    "transformed_dataset_val = GeneralDataset(csv_file=csv_files['val'],\n",
    "                                         root_dir=image_dir,\n",
    "                                         genomic_csv_file = genomic_csv_files['val'],\n",
    "                                         transform=data_transforms['val'],\n",
    "                                         seg_transform=data_transforms['seg'],\n",
    "                                         label='cluster',\n",
    "                                         classes=classes,\n",
    "                                         dataformat=dataformat,\n",
    "                                         returndims=resize_shape,\n",
    "                                         brats2tcia_df=brats2tcia_df,\n",
    "                                         modality=None)\n",
    "\n",
    "\n",
    "transformed_dataset_data = GeneralDataset(csv_file=csv_files['data'],\n",
    "                                         root_dir=image_dir,\n",
    "                                          genomic_csv_file = genomic_csv_files['data'],\n",
    "                                         transform=data_transforms['data'],\n",
    "                                          seg_transform=data_transforms['seg'],\n",
    "                                         label='cluster',\n",
    "                                         classes=classes,\n",
    "                                         dataformat=dataformat,\n",
    "                                         returndims=resize_shape,\n",
    "                                         brats2tcia_df=brats2tcia_df,\n",
    "                                         modality=None)\n",
    "\n",
    "image_datasets = {'train':transformed_dataset_train, \n",
    "                  'val':transformed_dataset_val,\n",
    "                  'data':transformed_dataset_data}\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(image_datasets['train'], batch_size=train_batch_size, shuffle=shuffle, num_workers=4)\n",
    "dataloader_val = DataLoader(image_datasets['val'], batch_size=val_batch_size, shuffle=shuffle, num_workers=4)\n",
    "dataloader_data = DataLoader(image_datasets['data'], batch_size=data_batch_size, shuffle=shuffle_data, num_workers=4)\n",
    "\n",
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val, 'data':dataloader_data}\n",
    "\n",
    "dataset_sizes = {'train':len(image_datasets['train']), \n",
    "                 'val':len(image_datasets['val']), \n",
    "                 'data':len(image_datasets['data'])}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtype_dict = {0:'wildtype', 1:'val'}\n",
    "# for i, data in enumerate(dataloaders['train']):\n",
    "#     (inputs, seg_image, genomic_data, seg_probs), labels,(OS, event), bratsID = data\n",
    "#     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#     print(np.unique(seg_probs))\n",
    "    \n",
    "# #     seg_image2 = seg_probs[0].max(1)[1].data.byte().cpu().numpy()\n",
    "    \n",
    "#     print('**', inputs.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboad: 3D_idh_croppedraw_64x64x64_bs-4_newdataloader_only-gt-idh\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "img_dims = str(resize_shape[0]) + 'x' + str(resize_shape[1]) + 'x' + str(resize_shape[2])\n",
    "model_outfile_dir = '3D_' + task + '_croppedraw_' + img_dims +'_bs-' + str(train_batch_size) + '_newdataloader_only-gt-idh'\n",
    "print('tensorboad:', model_outfile_dir)\n",
    "writer = SummaryWriter('runs1/'+model_outfile_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss weights: tensor([1.0364, 1.0000], device='cuda:0')\n",
      "Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training . . . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"../datasets.py\", line 142, in __getitem__\n    image, seg_image, seg_image_wt = modality3D(mr_path_dict=mr_path_dict, bb=bb, modality=self.modality)\n  File \"../format_data.py\", line 336, in modality3D\n    img = nib.load(mr_path_dict[modality])\nKeyError: None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9c3652ed6d31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     model, best_wts, best_auc, best_acc, best_auc_acc = train(model=esp_model, \n\u001b[0m\u001b[1;32m     39\u001b[0m                        \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                        \u001b[0mdata_transforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/fh/fast/holland_e/grp/HollandLabShared/Nicholas/repos/glioma_mtl/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, data_transforms, criterion, optimizer, scheduler, dataset_sizes, writer, num_epochs, verbose, device, channels, classes, pad, resize_shape, volume_val, weight_dir, weight_outfile_prefix)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mnum_samples_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenomic_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbratsID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"../datasets.py\", line 142, in __getitem__\n    image, seg_image, seg_image_wt = modality3D(mr_path_dict=mr_path_dict, bb=bb, modality=self.modality)\n  File \"../format_data.py\", line 336, in modality3D\n    img = nib.load(mr_path_dict[modality])\nKeyError: None\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cluster_df = csv_files['train'][task]\n",
    "_, cnts = np.unique(cluster_df, return_counts=True)\n",
    "loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts)\n",
    "loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "print('loss weights:', loss_weights)\n",
    "\n",
    "\n",
    "from train import train\n",
    "from models.Models import SegModel\n",
    "best_auc_list, best_acc_list, best_auc_acc_list = [], [], []\n",
    "epochs = 50\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    print('Iteration', i)\n",
    "\n",
    "    best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "    esp_model = SegModel(best_model_loc=best_model_loc, inp_res = resize_shape, num_classes=num_classes)\n",
    "    esp_model = esp_model.to(device=device)\n",
    "\n",
    "    optimizer_esp = optim.Adam(esp_model.parameters(), lr=0.0005) # change to adam\n",
    "    exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_esp, \n",
    "                                                         mode='min', \n",
    "                                                         factor=0.1, \n",
    "                                                         patience=10, # number of epochs with no change \n",
    "                                                         verbose=True, \n",
    "                                                         threshold=0.0001, \n",
    "                                                         threshold_mode='rel', \n",
    "                                                         cooldown=0, \n",
    "                                                         min_lr=0, \n",
    "                                                         eps=1e-08)\n",
    "\n",
    "\n",
    "    model, best_wts, best_auc, best_acc, best_auc_acc = train(model=esp_model, \n",
    "                       dataloaders=dataloaders,\n",
    "                       data_transforms=data_transforms,\n",
    "                       criterion = criterion, \n",
    "                       optimizer=optimizer_esp, \n",
    "                       scheduler=exp_scheduler,\n",
    "                       writer=writer,\n",
    "                       num_epochs=epochs, \n",
    "                       verbose=False, \n",
    "                       device=device,\n",
    "                       dataset_sizes=dataset_sizes,\n",
    "                       channels=1,\n",
    "                       resize_shape=resize_shape,\n",
    "                       classes=class_names,\n",
    "                       weight_outfile_prefix=model_outfile_dir)\n",
    "    del esp_model\n",
    "    del model\n",
    "    \n",
    "    best_auc_list.append(best_auc)\n",
    "    best_acc_list.append(best_acc)\n",
    "    best_auc_acc_list.append(best_auc_acc)\n",
    "    \n",
    "    \n",
    "    results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "            \n",
    "    if not os.path.exists('../model_weights/results/'):\n",
    "        os.makedirs('../model_weights/results/')\n",
    "            \n",
    "    with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(best_auc_list, fp)\n",
    "\n",
    "    with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(best_acc_list, fp)\n",
    "    \n",
    "    with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(best_auc_acc_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Two bad epochs (should redo) (March 8th)\n",
    "\n",
    "50 epoch; 10 loops\n",
    "\n",
    "3D_idh_croppedraw_64x64x64_bs-4_newdataloader\n",
    "\n",
    "    auc std: 0.08738088661629888\n",
    "    auc mean: 0.8121527777777778\n",
    "    \n",
    "    acc std: 0.08901614275582172\n",
    "    acc mean: 0.7819444444444444\n",
    "---\n",
    "tensorboad: 3D_idh_croppedraw_64x64x64_bs-4_newdataloader_only-gt-idh (wrong optimizer!!)\n",
    "\n",
    "    auc std: 0.02907397270859894\n",
    "    auc mean: 0.871875\n",
    "    \n",
    "    acc std: 0.046718130289025175\n",
    "    acc mean: 0.8065972222222222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(best_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(best_auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(best_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [ 0.8217592592592593,\n",
    " 0.8668981481481481,\n",
    " 0.8796296296296297,\n",
    " 0.8263888888888888,\n",
    " 0.8518518518518519,\n",
    " 0.861111111111111,\n",
    " 0.8553240740740741,\n",
    " 0.8726851851851852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
