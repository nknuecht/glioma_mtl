{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- genomic data needs to be PCA and split up into train/val/data\n",
    "- get rid of acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# can revome the lines that need these\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import utils\n",
    "\n",
    "from datasets import GeneralDataset\n",
    "# import Transforms as myTransforms\n",
    "from Transforms import get_transformations\n",
    "from utils import get_data_splits, get_input_params\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose parameters\n",
    "- 4 channel full resolution:\n",
    "```python\n",
    "    task \\in ['idh', '1p19q']\n",
    "    dataformat \\in ['raw3D', 'crop3Dslice', 'modality3D']\n",
    "    modality \\in ['t1ce', 'flair', 't2', 't1', 't1ce-t1']\n",
    "    mtl \\in [True, False]\n",
    "    include_genomic_data \\in [True, False]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'idh'\n",
    "dataformat = 'crop3Dslice'\n",
    "modality = 't1ce' # only relevent for 'modality3D' dataformat\n",
    "mtl = True\n",
    "include_genomic_data = False # don't include genomic data\n",
    "\n",
    "null_genomic = not include_genomic_data\n",
    "\n",
    "dataformat, channels, resize_shape = get_input_params(dataformat)\n",
    "\n",
    "old_labels = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:\t\t idh\n",
      "mtl:\t\t True\n",
      "dataformat:\t mtl_cropped\n",
      "channels:\t 4\n",
      "modality:\t t1ce\n",
      "resize_shape:\t (64, 64, 64)\n",
      "null_genomic:\t True\n",
      "old_labels:\t True\n"
     ]
    }
   ],
   "source": [
    "print('task:\\t\\t', task)\n",
    "print('mtl:\\t\\t', mtl)\n",
    "print('dataformat:\\t', dataformat)\n",
    "print('channels:\\t', channels)\n",
    "print('modality:\\t', modality)\n",
    "print('resize_shape:\\t', resize_shape)\n",
    "print('null_genomic:\\t', null_genomic)\n",
    "print('old_labels:\\t', old_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 467\n"
     ]
    }
   ],
   "source": [
    "# MRI directory\n",
    "image_dir = '../data/all_brats_scans/'\n",
    "\n",
    "# metadata for all brats (including tcia) data\n",
    "best_model_loc = '../pretrained/espnet_3d_brats.pth' # segmentation model weights\n",
    "glioma_metadata_df = pd.read_csv('../data/glioma_metadata.csv', index_col=0) # metadata file\n",
    "if old_labels:\n",
    "    glioma_metadata_df.loc[['Brats18_TCIA09_462_1', 'Brats18_TCIA10_236_1'], 'idh'] = 1 ######\n",
    "\n",
    "# get training splits\n",
    "train_df, val_df, classes = get_data_splits(metadata_df=glioma_metadata_df, task=task, mtl=mtl)\n",
    "\n",
    "# map between brats dataset and tcia data (tcia data is avalible for a subset of the brats patients)\n",
    "brats2tcia_df = glioma_metadata_df['tciaID']\n",
    "# brats2tcia_df = pd.read_csv('../../miccai_clean/data/brats2tcia_df_542x1.csv', index_col=0)\n",
    "\n",
    "# these are labeled files (they were paths in old dataloader) but they are dataframes\n",
    "labels_dict = {'train':train_df, 'val':val_df, 'data':glioma_metadata_df}\n",
    "\n",
    "genomic_data_dict = {'train':'../data/MGL/MGL_235x50.csv', 'val':'../data/MGL/MGL_235x50.csv'}\n",
    "\n",
    "label = task\n",
    "\n",
    "print('Train size', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wildtype', 'mutant']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size\n",
    "train_transformations, seg_transformations, val_transformations = get_transformations(channels=channels, \n",
    "                                                                                      resize_shape=resize_shape, \n",
    "                                                                                      prob_voxel_zero=0.2, \n",
    "                                                                                      prob_true=0.8, \n",
    "                                                                                      prob_channel_zero=0.5,\n",
    "                                                                                      mtl=False)\n",
    "data_transforms = {'train': train_transformations, 'val':   val_transformations}\n",
    "                   \n",
    "transformed_dataset_train = GeneralDataset(csv_file=train_df, ## change this from \"csv_file\"\n",
    "                                           root_dir=image_dir,\n",
    "                                           genomic_csv_file = genomic_data_dict['train'],\n",
    "                                           transform=train_transformations,\n",
    "                                           seg_transform=seg_transformations,\n",
    "                                           label=label,\n",
    "                                           classes=classes,\n",
    "                                           dataformat=dataformat,\n",
    "                                           returndims=resize_shape,\n",
    "                                           brats2tcia_df=brats2tcia_df,\n",
    "                                           null_genomic = null_genomic,\n",
    "                                           pretrained=best_model_loc,\n",
    "                                           modality=modality)\n",
    "\n",
    "transformed_dataset_val = GeneralDataset(csv_file=val_df,\n",
    "                                         root_dir=image_dir,\n",
    "                                         genomic_csv_file = genomic_data_dict['val'],\n",
    "                                         transform=val_transformations,\n",
    "                                         seg_transform=seg_transformations,\n",
    "                                         label=label,\n",
    "                                         classes=classes,\n",
    "                                         dataformat=dataformat,\n",
    "                                         returndims=resize_shape,\n",
    "                                         brats2tcia_df=brats2tcia_df,\n",
    "                                         null_genomic = null_genomic,\n",
    "                                         pretrained=best_model_loc,\n",
    "                                         modality=modality)\n",
    "\n",
    "\n",
    "image_datasets = {'train':transformed_dataset_train, 'val':transformed_dataset_val}\n",
    "\n",
    "train_batch_size, val_batch_size = 4, 4\n",
    "dataloader_train = DataLoader(image_datasets['train'], batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "dataloader_val = DataLoader(image_datasets['val'], batch_size=val_batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val}\n",
    "dataset_sizes = {'train':len(image_datasets['train']), 'val':len(image_datasets['val'])}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visulize training (or validation) data\n",
    "# for i, data in enumerate(dataloaders['train']):\n",
    "#     # data batch\n",
    "#     (image, seg_image, genomic_data, seg_probs), label, (OS, OS_EVENT), bratsID = data\n",
    "#     # print scan ID\n",
    "#     print(bratsID[0])\n",
    "    \n",
    "#     # format MRI images (slices of volumetric input)\n",
    "#     img = image[0,:, :, :, int(image.shape[-1]/2)].squeeze()\n",
    "#     img = utils.make_grid(img)\n",
    "#     img = img.detach().cpu().numpy()\n",
    "    \n",
    "#     # plot images\n",
    "#     plt.figure(figsize=(15, 8))\n",
    "#     img_list = [img[i].T for i in range(channels)] # 1 image per channel\n",
    "#     plt.imshow(np.hstack(img_list), cmap='Greys_r')\n",
    "#     plt.show()\n",
    "\n",
    "#     ## plot segmentation mask ##\n",
    "#     seg_img = seg_image[0, :, :, :, int(seg_image.shape[-1]/2)].squeeze()\n",
    "#     seg_img = utils.make_grid(seg_img).detach().cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(4, 4))\n",
    "#     plt.imshow(np.hstack([seg_img[0].T]), cmap='Greys_r')\n",
    "#     plt.show()\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboad: 3D_idh_t1ce_mtl-True_64x64x64_genomic-False\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "img_dims = str(resize_shape[0]) + 'x' + str(resize_shape[1]) + 'x' + str(resize_shape[2])\n",
    "model_outfile_dir = '3D_' + task + '_' + modality+'_mtl-' + str(mtl) + '_' \\\n",
    "                    + img_dims + '_genomic-' + str(include_genomic_data)\n",
    "print('tensorboad:', model_outfile_dir)\n",
    "writer = SummaryWriter('runs1/'+model_outfile_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss weights: tensor([1.0364, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "label_df = train_df.loc[train_df[task].isin([0,1])][task]\n",
    "_, cnts = np.unique(label_df, return_counts=True)\n",
    "loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts)\n",
    "loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "print('loss weights:', loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:\t\t idh\n",
      "mtl:\t\t True\n",
      "dataformat:\t mtl_cropped\n",
      "channels:\t 4\n",
      "modality:\t t1ce\n",
      "resize_shape:\t (64, 64, 64)\n",
      "null_genomic:\t True\n",
      "old_labels:\t True\n"
     ]
    }
   ],
   "source": [
    "print('task:\\t\\t', task)\n",
    "print('mtl:\\t\\t', mtl)\n",
    "print('dataformat:\\t', dataformat)\n",
    "print('channels:\\t', channels)\n",
    "print('modality:\\t', modality)\n",
    "print('resize_shape:\\t', resize_shape)\n",
    "print('null_genomic:\\t', null_genomic)\n",
    "print('old_labels:\\t', old_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nick_mtl_model import GBMNetMTL\n",
    "import os\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "def mtl_experiment(dataloaders,\n",
    "                   data_transforms,\n",
    "                   dataset_sizes,\n",
    "                   best_model_loc, \n",
    "                   best_auc_list,\n",
    "                   best_acc_list,\n",
    "                   model_outfile_dir,\n",
    "                   num_classes,channels, loss_weights,\n",
    "                   seg_4class_weights,\n",
    "                   seg_2class_weights,\n",
    "                   seg_loss_weight,\n",
    "                   surv_loss_weight,\n",
    "                   device,\n",
    "                   brats_seg_ids,\n",
    "                   writer,\n",
    "                   model_weights_dir='../model_weights/results/',\n",
    "                   epochs=50,\n",
    "                   standard_unlabled_loss=True,\n",
    "                   fusion_net_flag=True,\n",
    "                   modality=None,\n",
    "                   take_surv_loss=False,\n",
    "                   seg_classes=4,g_in_features=50,g_out_features=128):\n",
    "    \n",
    "\n",
    "    gbm_net = GBMNetMTL(g_in_features=g_in_features, \n",
    "                        g_out_features=g_out_features, \n",
    "                        n_classes=num_classes, \n",
    "                        n_volumes=channels, \n",
    "                        seg_classes=seg_classes, \n",
    "                        pretrained=best_model_loc, \n",
    "                        class_loss_weights = loss_weights,\n",
    "                        seg_4class_weights=seg_4class_weights,\n",
    "                        seg_2class_weights=seg_2class_weights,\n",
    "                        seg_loss_scale=seg_loss_weight,\n",
    "                        surv_loss_scale=surv_loss_weight,\n",
    "                        device = device,\n",
    "                        brats_seg_ids=brats_seg_ids,\n",
    "                        standard_unlabled_loss=standard_unlabled_loss,\n",
    "                        fusion_net_flag=fusion_net_flag,\n",
    "                        modality=modality,\n",
    "                        take_surv_loss=take_surv_loss)\n",
    "    gbm_net = gbm_net.to(device)\n",
    "\n",
    "    optimizer_gbmnet = optim.Adam(gbm_net.parameters(), lr=0.0005) # change to adami\n",
    "\n",
    "    exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_gbmnet, \n",
    "                                                         mode='min', \n",
    "                                                         factor=0.1, \n",
    "                                                         patience=10, # number of epochs with no change \n",
    "                                                         verbose=True, \n",
    "                                                         threshold=0.0001, \n",
    "                                                         threshold_mode='rel', \n",
    "                                                         cooldown=0, \n",
    "                                                         min_lr=0, \n",
    "                                                         eps=1e-08)\n",
    "\n",
    "    from train_mtl import train\n",
    "    model, best_wts, best_auc, best_acc, best_auc_acc = train(model=gbm_net, \n",
    "                   dataloaders=dataloaders,\n",
    "                   data_transforms=data_transforms,\n",
    "                   optimizer=optimizer_gbmnet, \n",
    "                   scheduler=exp_scheduler,\n",
    "                   writer=writer,\n",
    "                   num_epochs=epochs, \n",
    "                   verbose=False, \n",
    "                   device=device,\n",
    "                   dataset_sizes=dataset_sizes,\n",
    "                   channels=channels,\n",
    "                   classes=class_names,\n",
    "                   weight_outfile_prefix=model_outfile_dir,\n",
    "                   pad=0)\n",
    "\n",
    "\n",
    "    del gbm_net\n",
    "    del model\n",
    "\n",
    "    best_auc_list.append(best_auc)\n",
    "    best_acc_list.append(best_acc)\n",
    "#     best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "    if not os.path.exists(model_weights_dir):\n",
    "        os.makedirs(model_weights_dir)\n",
    "\n",
    "    results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "    with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(best_auc_list, fp)\n",
    "    with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(best_acc_list, fp)\n",
    "    with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(best_auc_acc_list, fp)\n",
    "        \n",
    "    return best_auc_list, best_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:\t\t idh\n",
      "mtl:\t\t True\n",
      "dataformat:\t mtl_cropped\n",
      "channels:\t 4\n",
      "modality:\t t1ce\n",
      "resize_shape:\t (64, 64, 64)\n",
      "null_genomic:\t True\n",
      "old_labels:\t True\n",
      "mtl\n",
      "seg_4class_weights: tensor([  1.0000, 124.3847,  60.4277, 188.0025], device='cuda:0')\n",
      "seg_2class_weights: tensor([ 1.0000, 33.4366], device='cuda:0')\n",
      "Iteration 0\n",
      "GBMNet!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85549e1b88db44a0a39cb1f7fb5494ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best AUC-acc average:\t 0.6452546296296297 \tin epoch 0\n",
      "New Best Dice:\t 0.4975407019734549 \tin epoch 0\n",
      "New Best ACC:\t 0.5 \tin epoch 0\n",
      "New Best ACC:\t 0.572337962962963 \tin epoch 1\n",
      "New Best AUC-acc average:\t 0.6458333333333334 \tin epoch 2\n",
      "New Best ACC:\t 0.6643518518518519 \tin epoch 2\n",
      "New Best AUC:\t 0.6990740740740741 \tin epoch 6\n",
      "New Best AUC-acc average:\t 0.7413194444444444 \tin epoch 7\n",
      "New Best AUC:\t 0.7800925925925926 \tin epoch 7\n",
      "New Best ACC:\t 0.7025462962962963 \tin epoch 7\n",
      "New Best AUC-acc average:\t 0.7482638888888888 \tin epoch 9\n",
      "New Best AUC:\t 0.78125 \tin epoch 9\n",
      "New Best ACC:\t 0.7152777777777778 \tin epoch 9\n",
      "New Best AUC:\t 0.7905092592592593 \tin epoch 11\n",
      "New Best AUC-acc average:\t 0.7607060185185185 \tin epoch 12\n",
      "New Best AUC:\t 0.8043981481481481 \tin epoch 12\n",
      "New Best ACC:\t 0.7170138888888888 \tin epoch 12\n",
      "New Best AUC:\t 0.8425925925925927 \tin epoch 13\n",
      "New Best AUC-acc average:\t 0.8038194444444444 \tin epoch 14\n",
      "New Best ACC:\t 0.7824074074074074 \tin epoch 14\n",
      "Epoch    26: reducing learning rate of group 0 to 5.0000e-05.\n",
      "New Best AUC-acc average:\t 0.8078703703703705 \tin epoch 39\n",
      "New Best AUC-acc average:\t 0.8087384259259258 \tin epoch 47\n",
      "New Best ACC:\t 0.7922453703703703 \tin epoch 47\n",
      "New Best ACC:\t 0.8136574074074074 \tin epoch 48\n",
      "\n",
      "Finished Training\n",
      "Iteration 1\n",
      "GBMNet!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bdaeea606345e4968c98c791652d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best AUC-acc average:\t 0.6099537037037037 \tin epoch 0\n",
      "New Best Dice:\t 0.4910436970977737 \tin epoch 0\n",
      "New Best ACC:\t 0.5 \tin epoch 0\n",
      "New Best AUC-acc average:\t 0.6631944444444444 \tin epoch 2\n",
      "New Best AUC-acc average:\t 0.7540509259259258 \tin epoch 3\n",
      "New Best ACC:\t 0.7025462962962963 \tin epoch 3\n",
      "New Best AUC:\t 0.7743055555555556 \tin epoch 6\n",
      "New Best ACC:\t 0.7170138888888888 \tin epoch 6\n",
      "New Best AUC:\t 0.8217592592592592 \tin epoch 9\n",
      "New Best AUC:\t 0.8333333333333334 \tin epoch 10\n",
      "New Best AUC-acc average:\t 0.7881944444444444 \tin epoch 12\n",
      "New Best AUC:\t 0.8506944444444444 \tin epoch 12\n",
      "New Best ACC:\t 0.7256944444444444 \tin epoch 12\n",
      "New Best AUC-acc average:\t 0.8454861111111112 \tin epoch 14\n",
      "New Best AUC:\t 0.8842592592592592 \tin epoch 14\n",
      "New Best ACC:\t 0.806712962962963 \tin epoch 14\n",
      "Epoch    26: reducing learning rate of group 0 to 5.0000e-05.\n",
      "New Best ACC:\t 0.8078703703703703 \tin epoch 41\n",
      "Epoch    47: reducing learning rate of group 0 to 5.0000e-06.\n",
      "\n",
      "Finished Training\n",
      "Iteration 2\n",
      "GBMNet!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277604cdee714e76b33603049d011faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best AUC-acc average:\t 0.78125 \tin epoch 0\n",
      "New Best Dice:\t 0.46366895978997696 \tin epoch 0\n",
      "New Best ACC:\t 0.7280092592592593 \tin epoch 0\n",
      "New Best AUC:\t 0.6585648148148149 \tin epoch 6\n",
      "New Best AUC-acc average:\t 0.8012152777777778 \tin epoch 7\n",
      "New Best AUC:\t 0.8414351851851852 \tin epoch 7\n",
      "New Best ACC:\t 0.7609953703703703 \tin epoch 7\n",
      "New Best AUC-acc average:\t 0.8090277777777778 \tin epoch 19\n",
      "New Best ACC:\t 0.7951388888888888 \tin epoch 19\n",
      "New Best AUC-acc average:\t 0.8266782407407407 \tin epoch 21\n",
      "New Best AUC:\t 0.8483796296296297 \tin epoch 21\n",
      "New Best ACC:\t 0.8049768518518519 \tin epoch 21\n",
      "New Best AUC:\t 0.8541666666666666 \tin epoch 30\n",
      "Epoch    33: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    44: reducing learning rate of group 0 to 5.0000e-06.\n",
      "\n",
      "Finished Training\n",
      "Iteration 3\n",
      "GBMNet!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8a4e1fda094c34879f6a61bb7d7c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best AUC-acc average:\t 0.3489583333333333 \tin epoch 0\n",
      "New Best Dice:\t 0.4512268618839788 \tin epoch 0\n",
      "New Best ACC:\t 0.5 \tin epoch 0\n",
      "New Best AUC-acc average:\t 0.7337962962962963 \tin epoch 1\n",
      "New Best Dice:\t 0.48294813483769444 \tin epoch 1\n",
      "New Best ACC:\t 0.6631944444444444 \tin epoch 1\n",
      "New Best ACC:\t 0.6898148148148149 \tin epoch 3\n",
      "New Best AUC:\t 0.7372685185185185 \tin epoch 6\n",
      "New Best AUC-acc average:\t 0.7511574074074074 \tin epoch 11\n",
      "New Best AUC:\t 0.787037037037037 \tin epoch 11\n",
      "New Best ACC:\t 0.7152777777777778 \tin epoch 11\n",
      "New Best AUC-acc average:\t 0.7997685185185186 \tin epoch 12\n",
      "New Best AUC:\t 0.8159722222222222 \tin epoch 12\n",
      "New Best ACC:\t 0.7835648148148149 \tin epoch 12\n",
      "New Best AUC-acc average:\t 0.8064236111111112 \tin epoch 15\n",
      "New Best AUC:\t 0.8587962962962964 \tin epoch 15\n",
      "New Best AUC-acc average:\t 0.8327546296296297 \tin epoch 24\n",
      "New Best ACC:\t 0.8206018518518519 \tin epoch 24\n",
      "Epoch    30: reducing learning rate of group 0 to 5.0000e-05.\n",
      "New Best AUC-acc average:\t 0.8527199074074074 \tin epoch 30\n",
      "New Best AUC:\t 0.8634259259259259 \tin epoch 30\n",
      "New Best ACC:\t 0.8420138888888888 \tin epoch 30\n",
      "New Best AUC:\t 0.8657407407407408 \tin epoch 31\n",
      "New Best AUC-acc average:\t 0.8553240740740742 \tin epoch 34\n",
      "New Best ACC:\t 0.8449074074074074 \tin epoch 34\n",
      "Epoch    43: reducing learning rate of group 0 to 5.0000e-06.\n",
      "New Best AUC:\t 0.8738425925925926 \tin epoch 42\n",
      "\n",
      "Finished Training\n",
      "Iteration 4\n",
      "GBMNet!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e20a5777824448b9a75a26658ac60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best AUC-acc average:\t 0.4574652777777778 \tin epoch 0\n",
      "New Best Dice:\t 0.4991305803121085 \tin epoch 0\n",
      "New Best ACC:\t 0.5028935185185185 \tin epoch 0\n",
      "New Best AUC-acc average:\t 0.7734375 \tin epoch 1\n",
      "New Best ACC:\t 0.7366898148148149 \tin epoch 1\n",
      "New Best AUC-acc average:\t 0.8182870370370371 \tin epoch 6\n",
      "New Best AUC:\t 0.8611111111111112 \tin epoch 6\n",
      "New Best ACC:\t 0.775462962962963 \tin epoch 6\n"
     ]
    }
   ],
   "source": [
    "print('task:\\t\\t', task)\n",
    "print('mtl:\\t\\t', mtl)\n",
    "print('dataformat:\\t', dataformat)\n",
    "print('channels:\\t', channels)\n",
    "print('modality:\\t', modality)\n",
    "print('resize_shape:\\t', resize_shape)\n",
    "print('null_genomic:\\t', null_genomic)\n",
    "print('old_labels:\\t', old_labels)\n",
    "\n",
    "best_auc_list, best_acc_list, best_auc_acc_list = [], [], []\n",
    "epochs = 50\n",
    "iterations = 10\n",
    "\n",
    "if mtl:\n",
    "    print('mtl')\n",
    "    seg_loss_weight = 1\n",
    "    surv_loss_weight = 1\n",
    "    \n",
    "\n",
    "    brats_seg_ids = glioma_metadata_df[glioma_metadata_df['gt_seg'] == 1].index\n",
    "\n",
    "    seg_4class_weights = np.load('../data/segmentation_notcropped_4-class_weights.npy')\n",
    "    seg_4class_weights = torch.FloatTensor(seg_4class_weights).to(device)\n",
    "    print('seg_4class_weights:', seg_4class_weights)\n",
    "\n",
    "    seg_2class_weights = np.load('../data/segmentation_notcropped_2-class_weights.npy')\n",
    "    seg_2class_weights = torch.FloatTensor(seg_2class_weights).to(device)\n",
    "    print('seg_2class_weights:', seg_2class_weights)\n",
    "    \n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print('Iteration', i)\n",
    "        \n",
    "        best_auc_list, best_acc_list = mtl_experiment(dataloaders=dataloaders,\n",
    "                                                      data_transforms=data_transforms,\n",
    "                                                      dataset_sizes=dataset_sizes,\n",
    "                                                      best_model_loc=best_model_loc, \n",
    "                                                      best_auc_list=best_auc_list,\n",
    "                                                      best_acc_list=best_acc_list,\n",
    "                                                      model_outfile_dir=model_outfile_dir,\n",
    "                                                      num_classes=num_classes,\n",
    "                                                      channels=channels, \n",
    "                                                      loss_weights=loss_weights,\n",
    "                                                      seg_4class_weights=seg_4class_weights,\n",
    "                                                      seg_2class_weights=seg_2class_weights,\n",
    "                                                      seg_loss_weight=seg_loss_weight,\n",
    "                                                      surv_loss_weight=surv_loss_weight,\n",
    "                                                      device=device,\n",
    "                                                      brats_seg_ids=brats_seg_ids,\n",
    "                                                      writer=writer,\n",
    "                                                      model_weights_dir='../model_weights/results/',\n",
    "                                                      epochs=50,\n",
    "                                                      standard_unlabled_loss=True,\n",
    "                                                      fusion_net_flag=True,\n",
    "                                                      modality=None,\n",
    "                                                      take_surv_loss=False,\n",
    "                                                      seg_classes=4, \n",
    "                                                      g_in_features=50,\n",
    "                                                      g_out_features=128)\n",
    "\n",
    "\n",
    "#         from models.nick_mtl_model import GBMNetMTL\n",
    "#         gbm_net = GBMNetMTL(g_in_features=50, \n",
    "#                             g_out_features=128, \n",
    "#                             n_classes=num_classes, \n",
    "#                             n_volumes=channels, \n",
    "#                             seg_classes=4, \n",
    "#                             pretrained=best_model_loc, \n",
    "#                             class_loss_weights = loss_weights,\n",
    "#                             seg_4class_weights=seg_4class_weights,\n",
    "#                             seg_2class_weights=seg_2class_weights,\n",
    "#                             seg_loss_scale=seg_loss_weight,\n",
    "#                             surv_loss_scale=surv_loss_weight,\n",
    "#                             device = device,\n",
    "#                             brats_seg_ids=brats_seg_ids,\n",
    "#                             standard_unlabled_loss=False,\n",
    "#                             fusion_net_flag=True,\n",
    "#                             modality=modality,\n",
    "#                             take_surv_loss=False)\n",
    "#         gbm_net = gbm_net.to(device)\n",
    "\n",
    "#         optimizer_gbmnet = optim.Adam(gbm_net.parameters(), lr=0.0005) # change to adami\n",
    " \n",
    "#         exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_gbmnet, \n",
    "#                                                              mode='min', \n",
    "#                                                              factor=0.1, \n",
    "#                                                              patience=10, # number of epochs with no change \n",
    "#                                                              verbose=True, \n",
    "#                                                              threshold=0.0001, \n",
    "#                                                              threshold_mode='rel', \n",
    "#                                                              cooldown=0, \n",
    "#                                                              min_lr=0, \n",
    "#                                                              eps=1e-08)\n",
    "\n",
    "#         from train_mtl import train\n",
    "#         model, best_wts, best_auc, best_acc, best_auc_acc = train(model=gbm_net, \n",
    "#                        dataloaders=dataloaders,\n",
    "#                        data_transforms=data_transforms,\n",
    "#                        optimizer=optimizer_gbmnet, \n",
    "#                        scheduler=exp_scheduler,\n",
    "#                        writer=writer,\n",
    "#                        num_epochs=epochs, \n",
    "#                        verbose=False, \n",
    "#                        device=device,\n",
    "#                        dataset_sizes=dataset_sizes,\n",
    "#                        channels=channels,\n",
    "#                        classes=class_names,\n",
    "#                        weight_outfile_prefix=model_outfile_dir,\n",
    "#                        pad=0)\n",
    "\n",
    "\n",
    "#         del gbm_net\n",
    "#         del model\n",
    "\n",
    "#         best_auc_list.append(best_auc)\n",
    "#         best_acc_list.append(best_acc)\n",
    "#         best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "#         if not os.path.exists('../model_weights/results/'):\n",
    "#             os.makedirs('../model_weights/results/')\n",
    "\n",
    "#         results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "#         with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "#             pickle.dump(best_auc_list, fp)\n",
    "#         with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "#             pickle.dump(best_acc_list, fp)\n",
    "#         with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "#             pickle.dump(best_auc_acc_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('task:\\t\\t', task)\n",
    "print('mtl:\\t\\t', mtl)\n",
    "print('dataformat:\\t', dataformat)\n",
    "print('channels:\\t', channels)\n",
    "print('modality:\\t', modality)\n",
    "print('resize_shape:\\t', resize_shape)\n",
    "print('null_genomic:\\t', null_genomic)\n",
    "print('old_labels:\\t', old_labels)\n",
    "\n",
    "best_auc_list, best_acc_list, best_auc_acc_list = [], [], []\n",
    "epochs = 50\n",
    "iterations = 10\n",
    "    \n",
    "if not mtl:\n",
    "    print('no mtl')\n",
    "    from train import train\n",
    "    if channels == 1:\n",
    "        for i in range(iterations):\n",
    "            print('Iteration', i)\n",
    "\n",
    "            from models.Models import SegModel\n",
    "            esp_model = SegModel(best_model_loc=best_model_loc, \n",
    "                                 inp_res = resize_shape, \n",
    "                                 num_classes=num_classes, \n",
    "                                 channels=4)\n",
    "\n",
    "            level0_weight = esp_model.espnet.level0.conv.weight[:, 0].unsqueeze(1)\n",
    "            esp_model.espnet.level0.conv = nn.Conv3d(1, 16, kernel_size=(7, 7, 7), \n",
    "                                                     stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "\n",
    "            esp_model.espnet.level0.conv.weight = nn.Parameter(level0_weight)\n",
    "            esp_model = esp_model.to(device=device)\n",
    "\n",
    "            optimizer_esp = optim.Adam(esp_model.parameters(), lr=0.0005) # change to adam\n",
    "\n",
    "            exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_esp, \n",
    "                                                                 mode='min', \n",
    "                                                                 factor=0.1, \n",
    "                                                                 patience=10, # number of epochs with no change \n",
    "                                                                 verbose=True, \n",
    "                                                                 threshold=0.0001, \n",
    "                                                                 threshold_mode='rel', \n",
    "                                                                 cooldown=0, \n",
    "                                                                 min_lr=0, \n",
    "                                                                 eps=1e-08)\n",
    "\n",
    "\n",
    "            model, best_wts, best_auc, best_acc, best_auc_acc = train(model=esp_model, \n",
    "                               dataloaders=dataloaders,\n",
    "                               data_transforms=data_transforms,\n",
    "                               criterion = criterion, \n",
    "                               optimizer=optimizer_esp, \n",
    "                               scheduler=exp_scheduler,\n",
    "                               writer=writer,\n",
    "                               num_epochs=epochs, \n",
    "                               verbose=False, \n",
    "                               device=device,\n",
    "                               dataset_sizes=dataset_sizes,\n",
    "                               channels=channels,\n",
    "                               resize_shape=resize_shape,\n",
    "                               classes=class_names,\n",
    "                               volume_val=False,\n",
    "                               weight_outfile_prefix=model_outfile_dir)\n",
    "            del esp_model\n",
    "            del model\n",
    "\n",
    "            best_auc_list.append(best_auc)\n",
    "            best_acc_list.append(best_acc)\n",
    "            best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "\n",
    "            if not os.path.exists('../model_weights/results/'):\n",
    "                os.makedirs('../model_weights/results/')\n",
    "\n",
    "            results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "            with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_list, fp)\n",
    "            with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_acc_list, fp)\n",
    "            with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_acc_list, fp)\n",
    "                \n",
    "    elif channels == 4:\n",
    "        criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "        for i in range(iterations):\n",
    "            print('Iteration', i)\n",
    "            from models.Models import SegModel\n",
    "            esp_model = SegModel(best_model_loc=best_model_loc, inp_res = resize_shape, num_classes=num_classes)\n",
    "\n",
    "            if channels == 1:\n",
    "                level0_weight = esp_model.espnet.level0.conv.weight[:, 0].unsqueeze(1)\n",
    "                esp_model.espnet.level0.conv = nn.Conv3d(1, 16, kernel_size=(7, 7, 7), \n",
    "                                                         stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "                esp_model.espnet.level0.conv.weight = nn.Parameter(level0_weight)\n",
    "\n",
    "\n",
    "\n",
    "            esp_model = esp_model.to(device=device)\n",
    "\n",
    "            optimizer_esp = optim.Adam(esp_model.parameters(), lr=0.0005) # change to adam\n",
    "            exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_esp, \n",
    "                                                                 mode='min', \n",
    "                                                                 factor=0.1, \n",
    "                                                                 patience=10, # number of epochs with no change \n",
    "                                                                 verbose=True, \n",
    "                                                                 threshold=0.0001, \n",
    "                                                                 threshold_mode='rel', \n",
    "                                                                 cooldown=0, \n",
    "                                                                 min_lr=0, \n",
    "                                                                 eps=1e-08)\n",
    "\n",
    "\n",
    "            \n",
    "            model, best_wts, best_auc, best_acc, best_auc_acc = train(model=esp_model, \n",
    "                               dataloaders=dataloaders,\n",
    "                               data_transforms=data_transforms,\n",
    "                               criterion = criterion, \n",
    "                               optimizer=optimizer_esp, \n",
    "                               scheduler=exp_scheduler,\n",
    "                               writer=writer,\n",
    "                               num_epochs=epochs, \n",
    "                               verbose=False, \n",
    "                               device=device,\n",
    "                               dataset_sizes=dataset_sizes,\n",
    "                               channels=1,\n",
    "                               resize_shape=resize_shape,\n",
    "                               classes=class_names,\n",
    "                               weight_outfile_prefix=model_outfile_dir)\n",
    "            del esp_model\n",
    "            del model\n",
    "\n",
    "            best_auc_list.append(best_auc)\n",
    "            best_acc_list.append(best_acc)\n",
    "            best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "\n",
    "            results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "\n",
    "            if not os.path.exists('../model_weights/results/'):\n",
    "                os.makedirs('../model_weights/results/')\n",
    "\n",
    "            with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_list, fp)\n",
    "\n",
    "            with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_acc_list, fp)\n",
    "\n",
    "            with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_acc_list, fp)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to combine\n",
    "- nick_mtl_model\n",
    "- train_mtl\n",
    "- joint_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pecify align_corners=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mtl.py:130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.upsample -> nn.functional.interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
