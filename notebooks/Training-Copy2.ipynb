{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- genomic data needs to be PCA and split up into train/val/data\n",
    "- get rid of acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# can revome the lines that need these\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import utils\n",
    "\n",
    "from datasets import GeneralDataset\n",
    "# import Transforms as myTransforms\n",
    "from Transforms import get_transformations\n",
    "from utils import get_data_splits, get_input_params\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose parameters\n",
    "- 4 channel full resolution:\n",
    "```python\n",
    "    task \\in ['idh', '1p19q']\n",
    "    dataformat \\in ['raw3D', 'crop3Dslice', 'modality3D']\n",
    "    modality \\in ['t1ce', 'flair', 't2', 't1', 't1ce-t1']\n",
    "    mtl \\in [True, False]\n",
    "    include_genomic_data \\in [True, False]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'idh'\n",
    "dataformat = 'crop3Dslice'\n",
    "modality = 't1ce' # only relevent for 'modality3D' dataformat\n",
    "mtl = True\n",
    "include_genomic_data = False # don't include genomic data\n",
    "\n",
    "null_genomic = not include_genomic_data\n",
    "\n",
    "dataformat, channels, resize_shape = get_input_params(dataformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:\t\t idh\n",
      "mtl:\t\t True\n",
      "dataformat:\t mtl_cropped\n",
      "channels:\t 4\n",
      "modality:\t t1ce\n",
      "resize_shape:\t (64, 64, 64)\n",
      "null_genomic:\t True\n"
     ]
    }
   ],
   "source": [
    "print('task:\\t\\t', task)\n",
    "print('mtl:\\t\\t', mtl)\n",
    "print('dataformat:\\t', dataformat)\n",
    "print('channels:\\t', channels)\n",
    "print('modality:\\t', modality)\n",
    "print('resize_shape:\\t', resize_shape)\n",
    "print('null_genomic:\\t', null_genomic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 467\n"
     ]
    }
   ],
   "source": [
    "# MRI directory\n",
    "image_dir = '../data/all_brats_scans/'\n",
    "\n",
    "# metadata for all brats (including tcia) data\n",
    "best_model_loc = '../pretrained/espnet_3d_brats.pth' # segmentation model weights\n",
    "glioma_metadata_df = pd.read_csv('../data/glioma_metadata.csv', index_col=0) # metadata file\n",
    "glioma_metadata_df.loc[['Brats18_TCIA09_462_1', 'Brats18_TCIA10_236_1'], 'idh'] = 1 ######\n",
    "\n",
    "# subtype_df = pd.read_csv('../../tcga_data_cleaning/data/processed_data/santa_cruz/patient_metadata/subtype_spreadsheet_1128-36.csv', index_col=0)\n",
    "# non_mol_gbm_idxs = subtype_df.loc[subtype_df['mol_gbm'] == -1].index\n",
    "# non_mol_df = glioma_metadata_df.loc[(glioma_metadata_df['tciaID'].isin(non_mol_gbm_idxs))\n",
    "#                       & (glioma_metadata_df['idh'] == 0)]\n",
    "# val_non_mol_idxs = non_mol_df.loc[non_mol_df['phase'] == 'val'].index\n",
    "# glioma_metadata_df.loc[val_non_mol_idxs, 'idh'] = -1\n",
    "\n",
    "\n",
    "# get training splits\n",
    "train_df, val_df, classes = get_data_splits(metadata_df=glioma_metadata_df, task=task, mtl=mtl)\n",
    "\n",
    "# map between brats dataset and tcia data (tcia data is avalible for a subset of the brats patients)\n",
    "brats2tcia_df = glioma_metadata_df['tciaID']\n",
    "# brats2tcia_df = pd.read_csv('../../miccai_clean/data/brats2tcia_df_542x1.csv', index_col=0)\n",
    "\n",
    "# these are labeled files (they were paths in old dataloader) but they are dataframes\n",
    "labels_dict = {'train':train_df, 'val':val_df, 'data':glioma_metadata_df}\n",
    "\n",
    "genomic_data_dict = {'train':'../data/MGL/MGL_235x50.csv', 'val':'../data/MGL/MGL_235x50.csv'}\n",
    "\n",
    "label = task\n",
    "\n",
    "print('Train size', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glioma_metadata_df = pd.read_csv('../data/glioma_metadata.csv', index_col=0) \n",
    "# glioma_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtype_df = pd.read_csv('../../tcga_data_cleaning/data/processed_data/santa_cruz/patient_metadata/subtype_spreadsheet_1128-36.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_mol_gbm_idxs = subtype_df.loc[subtype_df['mol_gbm'] == -1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_mol_df = glioma_metadata_df.loc[(glioma_metadata_df['tciaID'].isin(non_mol_gbm_idxs))\n",
    "#                       & (glioma_metadata_df['idh'] == 0)]\n",
    "# val_non_mol_idxs = non_mol_df.loc[non_mol_df['phase'] == 'val'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_non_mol_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wildtype', 'mutant']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size\n",
    "train_transformations, seg_transformations, val_transformations = get_transformations(channels=channels, \n",
    "                                                                                      resize_shape=resize_shape, \n",
    "                                                                                      prob_voxel_zero=0.2, \n",
    "                                                                                      prob_true=0.8, \n",
    "                                                                                      prob_channel_zero=0.5,\n",
    "                                                                                      mtl=False)\n",
    "data_transforms = {'train': train_transformations, 'val':   val_transformations}\n",
    "                   \n",
    "transformed_dataset_train = GeneralDataset(csv_file=train_df, ## change this from \"csv_file\"\n",
    "                                           root_dir=image_dir,\n",
    "                                           genomic_csv_file = genomic_data_dict['train'],\n",
    "                                           transform=train_transformations,\n",
    "                                           seg_transform=seg_transformations,\n",
    "                                           label=label,\n",
    "                                           classes=classes,\n",
    "                                           dataformat=dataformat,\n",
    "                                           returndims=resize_shape,\n",
    "                                           brats2tcia_df=brats2tcia_df,\n",
    "                                           null_genomic = null_genomic,\n",
    "                                           pretrained=best_model_loc,\n",
    "                                           modality=modality)\n",
    "\n",
    "transformed_dataset_val = GeneralDataset(csv_file=val_df,\n",
    "                                         root_dir=image_dir,\n",
    "                                         genomic_csv_file = genomic_data_dict['val'],\n",
    "                                         transform=val_transformations,\n",
    "                                         seg_transform=seg_transformations,\n",
    "                                         label=label,\n",
    "                                         classes=classes,\n",
    "                                         dataformat=dataformat,\n",
    "                                         returndims=resize_shape,\n",
    "                                         brats2tcia_df=brats2tcia_df,\n",
    "                                         null_genomic = null_genomic,\n",
    "                                         pretrained=best_model_loc,\n",
    "                                         modality=modality)\n",
    "\n",
    "\n",
    "image_datasets = {'train':transformed_dataset_train, 'val':transformed_dataset_val}\n",
    "\n",
    "train_batch_size, val_batch_size = 4, 4\n",
    "dataloader_train = DataLoader(image_datasets['train'], batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "dataloader_val = DataLoader(image_datasets['val'], batch_size=val_batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val}\n",
    "dataset_sizes = {'train':len(image_datasets['train']), 'val':len(image_datasets['val'])}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visulize training (or validation) data\n",
    "# for i, data in enumerate(dataloaders['train']):\n",
    "#     # data batch\n",
    "#     (image, seg_image, genomic_data, seg_probs), label, (OS, OS_EVENT), bratsID = data\n",
    "#     # print scan ID\n",
    "#     print(bratsID[0])\n",
    "    \n",
    "#     # format MRI images (slices of volumetric input)\n",
    "#     img = image[0,:, :, :, int(image.shape[-1]/2)].squeeze()\n",
    "#     img = utils.make_grid(img)\n",
    "#     img = img.detach().cpu().numpy()\n",
    "    \n",
    "#     # plot images\n",
    "#     plt.figure(figsize=(15, 8))\n",
    "#     img_list = [img[i].T for i in range(channels)] # 1 image per channel\n",
    "#     plt.imshow(np.hstack(img_list), cmap='Greys_r')\n",
    "#     plt.show()\n",
    "\n",
    "#     ## plot segmentation mask ##\n",
    "#     seg_img = seg_image[0, :, :, :, int(seg_image.shape[-1]/2)].squeeze()\n",
    "#     seg_img = utils.make_grid(seg_img).detach().cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(4, 4))\n",
    "#     plt.imshow(np.hstack([seg_img[0].T]), cmap='Greys_r')\n",
    "#     plt.show()\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboad: 3D_idh_t1ce_mtl-True_64x64x64_genomic-False\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "img_dims = str(resize_shape[0]) + 'x' + str(resize_shape[1]) + 'x' + str(resize_shape[2])\n",
    "model_outfile_dir = '3D_' + task + '_' + modality+'_mtl-' + str(mtl) + '_' \\\n",
    "                    + img_dims + '_genomic-' + str(include_genomic_data)\n",
    "print('tensorboad:', model_outfile_dir)\n",
    "writer = SummaryWriter('runs1/'+model_outfile_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss weights: tensor([1.0364, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "label_df = train_df.loc[train_df[task].isin([0,1])][task]\n",
    "_, cnts = np.unique(label_df, return_counts=True)\n",
    "loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts)\n",
    "loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "print('loss weights:', loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc_list, best_acc_list, best_auc_acc_list = [], [], []\n",
    "epochs = 50\n",
    "iterations = 10\n",
    "    \n",
    "if not mtl:\n",
    "    print('no mtl')\n",
    "    from train import train\n",
    "    if channels == 1:\n",
    "        for i in range(iterations):\n",
    "            print('Iteration', i)\n",
    "\n",
    "            from models.Models import SegModel\n",
    "\n",
    "            esp_model = SegModel(best_model_loc=best_model_loc, \n",
    "                                 inp_res = resize_shape, \n",
    "                                 num_classes=num_classes, \n",
    "                                 channels=4)\n",
    "\n",
    "            level0_weight = esp_model.espnet.level0.conv.weight[:, 0].unsqueeze(1)\n",
    "            esp_model.espnet.level0.conv = nn.Conv3d(1, 16, kernel_size=(7, 7, 7), \n",
    "                                                     stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "\n",
    "            esp_model.espnet.level0.conv.weight = nn.Parameter(level0_weight)\n",
    "            esp_model = esp_model.to(device=device)\n",
    "\n",
    "            optimizer_esp = optim.Adam(esp_model.parameters(), lr=0.0005) # change to adam\n",
    "        #     exp_scheduler = optim.lr_scheduler.StepLR(optimizer_esp, step_size=7, gamma=0.1)\n",
    "\n",
    "            exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_esp, \n",
    "                                                                 mode='min', \n",
    "                                                                 factor=0.1, \n",
    "                                                                 patience=10, # number of epochs with no change \n",
    "                                                                 verbose=True, \n",
    "                                                                 threshold=0.0001, \n",
    "                                                                 threshold_mode='rel', \n",
    "                                                                 cooldown=0, \n",
    "                                                                 min_lr=0, \n",
    "                                                                 eps=1e-08)\n",
    "\n",
    "\n",
    "            model, best_wts, best_auc, best_acc, best_auc_acc = train(model=esp_model, \n",
    "                               dataloaders=dataloaders,\n",
    "                               data_transforms=data_transforms,\n",
    "                               criterion = criterion, \n",
    "                               optimizer=optimizer_esp, \n",
    "                               scheduler=exp_scheduler,\n",
    "                               writer=writer,\n",
    "                               num_epochs=epochs, \n",
    "                               verbose=False, \n",
    "                               device=device,\n",
    "                               dataset_sizes=dataset_sizes,\n",
    "                               channels=1,\n",
    "                               resize_shape=resize_shape,\n",
    "                               classes=class_names,\n",
    "                               volume_val=False,\n",
    "                               weight_outfile_prefix=model_outfile_dir)\n",
    "            del esp_model\n",
    "            del model\n",
    "\n",
    "            best_auc_list.append(best_auc)\n",
    "            best_acc_list.append(best_acc)\n",
    "            best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "\n",
    "            if not os.path.exists('../model_weights/results/'):\n",
    "                os.makedirs('../model_weights/results/')\n",
    "\n",
    "            results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "            with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_list, fp)\n",
    "            with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_acc_list, fp)\n",
    "            with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_acc_list, fp)\n",
    "                \n",
    "    elif channels == 4:\n",
    "        criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "        for i in range(iterations):\n",
    "            print('Iteration', i)\n",
    "            from models.Models import SegModel\n",
    "            esp_model = SegModel(best_model_loc=best_model_loc, inp_res = resize_shape, num_classes=num_classes)\n",
    "\n",
    "            if channels == 1:\n",
    "                level0_weight = esp_model.espnet.level0.conv.weight[:, 0].unsqueeze(1)\n",
    "                esp_model.espnet.level0.conv = nn.Conv3d(1, 16, kernel_size=(7, 7, 7), \n",
    "                                                         stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "                esp_model.espnet.level0.conv.weight = nn.Parameter(level0_weight)\n",
    "\n",
    "\n",
    "\n",
    "            esp_model = esp_model.to(device=device)\n",
    "\n",
    "            optimizer_esp = optim.Adam(esp_model.parameters(), lr=0.0005) # change to adam\n",
    "            exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_esp, \n",
    "                                                                 mode='min', \n",
    "                                                                 factor=0.1, \n",
    "                                                                 patience=10, # number of epochs with no change \n",
    "                                                                 verbose=True, \n",
    "                                                                 threshold=0.0001, \n",
    "                                                                 threshold_mode='rel', \n",
    "                                                                 cooldown=0, \n",
    "                                                                 min_lr=0, \n",
    "                                                                 eps=1e-08)\n",
    "\n",
    "\n",
    "            \n",
    "            model, best_wts, best_auc, best_acc, best_auc_acc = train(model=esp_model, \n",
    "                               dataloaders=dataloaders,\n",
    "                               data_transforms=data_transforms,\n",
    "                               criterion = criterion, \n",
    "                               optimizer=optimizer_esp, \n",
    "                               scheduler=exp_scheduler,\n",
    "                               writer=writer,\n",
    "                               num_epochs=epochs, \n",
    "                               verbose=False, \n",
    "                               device=device,\n",
    "                               dataset_sizes=dataset_sizes,\n",
    "                               channels=1,\n",
    "                               resize_shape=resize_shape,\n",
    "                               classes=class_names,\n",
    "                               weight_outfile_prefix=model_outfile_dir)\n",
    "            del esp_model\n",
    "            del model\n",
    "\n",
    "            best_auc_list.append(best_auc)\n",
    "            best_acc_list.append(best_acc)\n",
    "            best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "\n",
    "            results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "\n",
    "            if not os.path.exists('../model_weights/results/'):\n",
    "                os.makedirs('../model_weights/results/')\n",
    "\n",
    "            with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_list, fp)\n",
    "\n",
    "            with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_acc_list, fp)\n",
    "\n",
    "            with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_acc_list, fp)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtl\n",
      "seg_4class_weights: tensor([  1.0000, 124.3847,  60.4277, 188.0025], device='cuda:0')\n",
      "seg_2class_weights: tensor([ 1.0000, 33.4366], device='cuda:0')\n",
      "Iteration 0\n",
      "GBMNet!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83554b4606540bdb65282f1be303ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best AUC-acc average:\t 0.7248263888888888 \tin epoch 0\n",
      "New Best Dice:\t 0.5224299565314197 \tin epoch 0\n",
      "New Best ACC:\t 0.6938657407407407 \tin epoch 0\n",
      "New Best AUC-acc average:\t 0.8023726851851851 \tin epoch 3\n",
      "New Best ACC:\t 0.7795138888888888 \tin epoch 3\n",
      "New Best AUC:\t 0.8136574074074074 \tin epoch 6\n",
      "New Best AUC:\t 0.8252314814814814 \tin epoch 7\n",
      "New Best AUC:\t 0.8472222222222222 \tin epoch 11\n",
      "New Best AUC-acc average:\t 0.8029513888888888 \tin epoch 12\n",
      "Epoch    32: reducing learning rate of group 0 to 5.0000e-05.\n",
      "New Best AUC-acc average:\t 0.8125 \tin epoch 43\n",
      "New Best ACC:\t 0.8009259259259259 \tin epoch 43\n",
      "\n",
      "Finished Training\n",
      "Iteration 1\n",
      "GBMNet!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdf5604f60040d28a35c9cfa696d8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best AUC-acc average:\t 0.5619212962962963 \tin epoch 0\n",
      "New Best Dice:\t 0.45274225641790894 \tin epoch 0\n",
      "New Best ACC:\t 0.5 \tin epoch 0\n",
      "New Best AUC-acc average:\t 0.7054398148148149 \tin epoch 1\n",
      "New Best Dice:\t 0.45439949146805564 \tin epoch 1\n",
      "New Best ACC:\t 0.6898148148148149 \tin epoch 1\n",
      "New Best Dice:\t 0.468387501011641 \tin epoch 2\n",
      "New Best AUC-acc average:\t 0.752025462962963 \tin epoch 5\n",
      "New Best AUC-acc average:\t 0.8153935185185185 \tin epoch 6\n",
      "New Best AUC:\t 0.8356481481481481 \tin epoch 6\n",
      "New Best ACC:\t 0.7951388888888888 \tin epoch 6\n",
      "New Best AUC-acc average:\t 0.8156828703703703 \tin epoch 9\n",
      "New Best AUC:\t 0.8518518518518519 \tin epoch 9\n",
      "Epoch    27: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    38: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 5.0000e-07.\n",
      "\n",
      "Finished Training\n",
      "Iteration 2\n",
      "GBMNet!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04190b9e6dd4ec1b7822085a58fa3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best AUC-acc average:\t 0.5387731481481481 \tin epoch 0\n",
      "New Best Dice:\t 0.5396393722905258 \tin epoch 0\n",
      "New Best ACC:\t 0.5381944444444444 \tin epoch 0\n",
      "New Best AUC-acc average:\t 0.6412037037037036 \tin epoch 2\n",
      "New Best AUC-acc average:\t 0.8003472222222222 \tin epoch 3\n",
      "New Best ACC:\t 0.7893518518518519 \tin epoch 3\n",
      "New Best AUC:\t 0.6168981481481481 \tin epoch 6\n",
      "New Best AUC:\t 0.7060185185185186 \tin epoch 7\n",
      "New Best AUC:\t 0.7256944444444444 \tin epoch 8\n",
      "New Best AUC:\t 0.8310185185185186 \tin epoch 9\n",
      "New Best AUC-acc average:\t 0.8200231481481481 \tin epoch 10\n",
      "New Best AUC:\t 0.8506944444444444 \tin epoch 10\n",
      "Epoch    22: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    33: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch    44: reducing learning rate of group 0 to 5.0000e-07.\n",
      "\n",
      "Finished Training\n",
      "Iteration 3\n",
      "GBMNet!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69baf1ddf0164ea482b78750baf20228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7523c4434e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_mtl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         model, best_wts, best_auc, best_acc, best_auc_acc = train(model=gbm_net, \n\u001b[0m\u001b[1;32m     57\u001b[0m                        \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                        \u001b[0mdata_transforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/fh/fast/holland_e/grp/HollandLabShared/Nicholas/repos/glioma_mtl/train_mtl.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, data_transforms, optimizer, scheduler, dataset_sizes, writer, num_epochs, verbose, device, channels, classes, pad, weight_dir, weight_outfile_prefix, pretrained)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# running_loss += loss.item() * inputs.size(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if mtl:\n",
    "    print('mtl')\n",
    "    seg_loss_weight = 1\n",
    "    surv_loss_weight = 1\n",
    "    \n",
    "\n",
    "    brats_seg_ids = glioma_metadata_df[glioma_metadata_df['gt_seg'] == 1].index\n",
    "\n",
    "    seg_4class_weights = np.load('../data/segmentation_notcropped_4-class_weights.npy')\n",
    "    seg_4class_weights = torch.FloatTensor(seg_4class_weights).to(device)\n",
    "    print('seg_4class_weights:', seg_4class_weights)\n",
    "\n",
    "    seg_2class_weights = np.load('../data/segmentation_notcropped_2-class_weights.npy')\n",
    "    seg_2class_weights = torch.FloatTensor(seg_2class_weights).to(device)\n",
    "    print('seg_2class_weights:', seg_2class_weights)\n",
    "    \n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print('Iteration', i)\n",
    "\n",
    "\n",
    "        from models.nick_mtl_model import GBMNetMTL\n",
    "        gbm_net = GBMNetMTL(g_in_features=50, \n",
    "                            g_out_features=128, \n",
    "                            n_classes=num_classes, \n",
    "                            n_volumes=channels, \n",
    "                            seg_classes=4, \n",
    "                            pretrained=best_model_loc, \n",
    "                            class_loss_weights = loss_weights,\n",
    "                            seg_4class_weights=seg_4class_weights,\n",
    "                            seg_2class_weights=seg_2class_weights,\n",
    "                            seg_loss_scale=seg_loss_weight,\n",
    "                            surv_loss_scale=surv_loss_weight,\n",
    "                            device = device,\n",
    "                            brats_seg_ids=brats_seg_ids,\n",
    "                            standard_unlabled_loss=True,\n",
    "                            fusion_net_flag=False,\n",
    "                            modality=modality,\n",
    "                            take_surv_loss=False)\n",
    "        gbm_net = gbm_net.to(device)\n",
    "\n",
    "        optimizer_gbmnet = optim.Adam(gbm_net.parameters(), lr=0.0005) # change to adami\n",
    " \n",
    "        exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_gbmnet, \n",
    "                                                             mode='min', \n",
    "                                                             factor=0.1, \n",
    "                                                             patience=10, # number of epochs with no change \n",
    "                                                             verbose=True, \n",
    "                                                             threshold=0.0001, \n",
    "                                                             threshold_mode='rel', \n",
    "                                                             cooldown=0, \n",
    "                                                             min_lr=0, \n",
    "                                                             eps=1e-08)\n",
    "\n",
    "        from train_mtl import train\n",
    "        model, best_wts, best_auc, best_acc, best_auc_acc = train(model=gbm_net, \n",
    "                       dataloaders=dataloaders,\n",
    "                       data_transforms=data_transforms,\n",
    "                       optimizer=optimizer_gbmnet, \n",
    "                       scheduler=exp_scheduler,\n",
    "                       writer=writer,\n",
    "                       num_epochs=epochs, \n",
    "                       verbose=False, \n",
    "                       device=device,\n",
    "                       dataset_sizes=dataset_sizes,\n",
    "                       channels=channels,\n",
    "                       classes=class_names,\n",
    "                       weight_outfile_prefix=model_outfile_dir,\n",
    "                       pad=0)\n",
    "\n",
    "\n",
    "        del gbm_net\n",
    "        del model\n",
    "\n",
    "        best_auc_list.append(best_auc)\n",
    "        best_acc_list.append(best_acc)\n",
    "        best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "        if not os.path.exists('../model_weights/results/'):\n",
    "            os.makedirs('../model_weights/results/')\n",
    "\n",
    "        results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "        with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(best_auc_list, fp)\n",
    "        with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(best_acc_list, fp)\n",
    "        with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(best_auc_acc_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  6 16:31:01 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 47%   65C    P2   183W / 250W |   7539MiB / 11019MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     27135      C   ...-GCCcore-9.3.0/bin/python     3761MiB |\n",
      "|    0   N/A  N/A     36172      C   ...-GCCcore-9.3.0/bin/python     3775MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11300"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7539+3761"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to combine\n",
    "- nick_mtl_model\n",
    "- train_mtl\n",
    "- joint_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pecify align_corners=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mtl.py:130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.upsample -> nn.functional.interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
