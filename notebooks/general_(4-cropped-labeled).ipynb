{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- genomic data needs to be PCA and split up into train/val/data\n",
    "- get rid of acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# can revome the lines that need these\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datasets import GeneralDataset\n",
    "import Transforms as myTransforms\n",
    "from utils import get_data_splits\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose parameters\n",
    "- 4 channel full resolution:\n",
    "```python\n",
    "    task \\in ['idh', '1p19q']\n",
    "    dataformat \\in ['raw3D', 'crop3Dslice', 'modality3D']\n",
    "    modality \\in ['t1ce', 'flair', 't2', 't1', 't1ce-t1']\n",
    "    mtl \\in [True, False]\n",
    "    include_genomic_data \\in [True, False]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'idh'\n",
    "dataformat = 'raw3D'\n",
    "mtl = True\n",
    "\n",
    "modality = None\n",
    "modality = 't1ce'\n",
    "\n",
    "# don't include genomic data\n",
    "include_genomic_data = False\n",
    "null_genomic = not include_genomic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_params(dataformat, mtl=True):\n",
    "    '''\n",
    "    This function returns a parameters used in our dataloader\n",
    "    Arguments\n",
    "    ---------\n",
    "    dataformat: one of 'raw3D', 'crop3Dslice', 'raw3D'\n",
    "               - 'modality3D' <- single MRI sequence cropped to tumor boundary (1 channel volume)\n",
    "               - 'crop3Dslice' <- 4 MRI sequences cropped to tumor boundary (4 channel volume)\n",
    "               - 'raw3D' <- 4 whole-brain MRI sequences (4 channel volume)\n",
    "    mtl: whether or not to use unlabeled MRI sequences; i.e., a choice between simple CNNs or MTL network\n",
    "\n",
    "    Outputs\n",
    "    ---------\n",
    "    dataformat:      Revise dataformat string if MTL is used\n",
    "    channels:        Input channels for MRI input\n",
    "    resize_shape:    Hard coded input sizes for indiviual MRI sequences: either (64, 64, 64) or (144, 144, 144)\n",
    "    '''    \n",
    "    try:\n",
    "        if dataformat == 'modality3D':\n",
    "            channels = 1\n",
    "            resize_shape = (64, 64, 64)\n",
    "            if mtl:\n",
    "                dataformat = 'modality3D_mtl'\n",
    "        elif dataformat == 'crop3Dslice':\n",
    "            channels = 4\n",
    "            resize_shape = (64, 64, 64)\n",
    "            if mtl:\n",
    "                dataformat = 'mtl_cropped'\n",
    "        elif dataformat == 'raw3D':\n",
    "            channels = 4\n",
    "            resize_shape = (144, 144, 144)\n",
    "            if mtl:\n",
    "                dataformat = 'raw3D_mtl'\n",
    "\n",
    "        return dataformat, channels, resize_shape\n",
    "    except:\n",
    "        print('Incorrect dataformat')\n",
    "        return _, _, _\n",
    "\n",
    "dataformat, channels, resize_shape = get_input_params(dataformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:\t\t idh\n",
      "mtl:\t\t True\n",
      "dataformat:\t raw3D_mtl\n",
      "channels:\t 4\n",
      "modality:\t t1ce\n",
      "resize_shape:\t (144, 144, 144)\n",
      "null_genomic:\t True\n"
     ]
    }
   ],
   "source": [
    "print('task:\\t\\t', task)\n",
    "print('mtl:\\t\\t', mtl)\n",
    "print('dataformat:\\t', dataformat)\n",
    "print('channels:\\t', channels)\n",
    "print('modality:\\t', modality)\n",
    "print('resize_shape:\\t', resize_shape)\n",
    "print('null_genomic:\\t', null_genomic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_splits(metadata_df, task='idh', mtl = False):\n",
    "    \n",
    "#     '''\n",
    "#     This function returns pandas dataframes containing training and validation indices and sample metadata\n",
    "#     Arguments\n",
    "#     ---------\n",
    "#     metadata_df:   For each sample this dataframe indicates\n",
    "#                         1) whether it is in the labeled training, unlabeled training set, or valiation set\n",
    "#                         2) its idh status, and 1p19q status\n",
    "#     task:          Either 'idh' or '1p19q'\n",
    "#     mtl:           If True, MRI data without IDH mutation or 1p/19q co-deletion labels will be included in the traing set\n",
    "    \n",
    "#     Outputs\n",
    "#     ---------\n",
    "#     train_df:      Dataframe of training samples\n",
    "#     val_df:        Dataframe of validation samples\n",
    "#     classes:       Names of numerical labels\n",
    "#     '''\n",
    "    \n",
    "#     # validation set\n",
    "#     if task == 'idh':\n",
    "#         classes = ['wildtype', 'mutant']\n",
    "#         val_df = glioma_metadata_df.loc[(glioma_metadata_df['phase'] == 'val') \n",
    "#                                         & (glioma_metadata_df[task].isin([0,1]))] # check whether IDH status known\n",
    "#     elif task == '1p19q':\n",
    "#         classes = ['non-codel', 'oligo']\n",
    "#         val_df = glioma_metadata_df.loc[glioma_metadata_df['phase'] == 'val']\n",
    "    \n",
    "#     # training set\n",
    "#     if mtl:\n",
    "#         train_df = glioma_metadata_df.loc[glioma_metadata_df['phase'].isin(['train', 'unlabeled train'])]\n",
    "#     else:\n",
    "#         train_df = glioma_metadata_df.loc[(glioma_metadata_df['phase'] == 'train') \n",
    "#                                           & (glioma_metadata_df[task].isin([0,1]))] # only labeled data (0/1)\n",
    "\n",
    "#     return train_df, val_df, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 467\n"
     ]
    }
   ],
   "source": [
    "# MRI directory\n",
    "image_dir = '../data/all_brats_scans/'\n",
    "\n",
    "# metadata for all brats (including tcia) data\n",
    "best_model_loc = '../pretrained/espnet_3d_brats.pth' # segmentation model weights\n",
    "glioma_metadata_df = pd.read_csv('../data/glioma_metadata.csv', index_col=0) # metadata file\n",
    "glioma_metadata_df.loc[['Brats18_TCIA09_462_1', 'Brats18_TCIA10_236_1'], 'idh'] = 1 ######\n",
    "\n",
    "# get training splits\n",
    "train_df, val_df, classes = get_data_splits(metadata_df=glioma_metadata_df, task=task, mtl=mtl)\n",
    "\n",
    "# map between brats dataset and tcia data (tcia data is avalible for a subset of the brats patients)\n",
    "brats2tcia_df = glioma_metadata_df['tciaID']\n",
    "# brats2tcia_df = pd.read_csv('../../miccai_clean/data/brats2tcia_df_542x1.csv', index_col=0)\n",
    "\n",
    "# these are labeled files (they were paths in old dataloader) but they are dataframes\n",
    "labels_dict = {'train':train_df, 'val':val_df, 'data':glioma_metadata_df}\n",
    "\n",
    "genomic_data_dict = {'train':'../data/MGL/MGL_235x50.csv', 'val':'../data/MGL/MGL_235x50.csv'}\n",
    "\n",
    "label = task\n",
    "\n",
    "print('Train size', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_transformations(channels, resize_shape, voxel_zero_prob=0.8, voxel_zero_rate=0.2, prob_zero_channel=0.5):\n",
    "#     if channels == 4:\n",
    "#         train_transformations = myTransforms.Compose([\n",
    "#                 myTransforms.MinMaxNormalize(),\n",
    "#                 myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "#                                           interpolation=1, \n",
    "#                                           channels=channels),\n",
    "#                 myTransforms.ZeroSprinkle(prob_zero=voxel_zero_rate, prob_true=voxel_zero_prob),\n",
    "#                 myTransforms.ZeroChannel(prob_zero=prob_zero_channel),\n",
    "#                 myTransforms.RandomFlip(),\n",
    "#                 myTransforms.ToTensor(),\n",
    "#             ])\n",
    "\n",
    "#         seg_transformations = myTransforms.Compose([\n",
    "#             myTransforms.ScaleToFixed((1, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "#                                           interpolation=0, \n",
    "#                                           channels=1),\n",
    "#                 myTransforms.ToTensor(),\n",
    "#             ])\n",
    "\n",
    "\n",
    "#         val_transformations = myTransforms.Compose([\n",
    "#                 myTransforms.MinMaxNormalize(),\n",
    "#                 myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "#                                           interpolation=1, \n",
    "#                                           channels=channels),\n",
    "#                 myTransforms.ToTensor(),\n",
    "#             ])\n",
    "#     elif channels == 1:\n",
    "#         # minimal data augmentation (you can add more)\n",
    "#         train_transformations = myTransforms.Compose([\n",
    "#                 myTransforms.MinMaxNormalize(),\n",
    "#                 myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2])),\n",
    "#                 myTransforms.ZeroSprinkle(prob_zero=voxel_zero_rate, prob_true=voxel_zero_prob),\n",
    "#                 myTransforms.ToTensor(),\n",
    "#             ])\n",
    "\n",
    "#         # segmentation masks have separate transformations (don't want to normalize)\n",
    "#         seg_transformations = myTransforms.Compose([\n",
    "#               myTransforms.ScaleToFixed((1, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "#                                           interpolation=0, \n",
    "#                                           channels=channels),\n",
    "#                 myTransforms.ToTensor(),\n",
    "#             ])\n",
    "\n",
    "#         val_transformations = myTransforms.Compose([\n",
    "#                 myTransforms.MinMaxNormalize(),\n",
    "#                 myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2])),\n",
    "#                 myTransforms.ToTensor(),\n",
    "#             ])\n",
    "        \n",
    "#     return train_transformations, seg_transformations, val_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wildtype', 'mutant']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size\n",
    "train_batch_size, val_batch_size = 4, 4\n",
    "\n",
    "if channels == 1:\n",
    "    interpolation = 1\n",
    "    # minimal data augmentation (you can add more)\n",
    "    train_transformations = myTransforms.Compose([\n",
    "            myTransforms.MinMaxNormalize(),\n",
    "            myTransforms.ScaleToFixed((1, resize_shape[0],resize_shape[1],resize_shape[2]),\n",
    "                                      interpolation=interpolation,\n",
    "                                      channels=channels), # 1 is also channels\n",
    "            myTransforms.ZeroSprinkle(prob_zero=0.2, prob_true=0.8),\n",
    "            myTransforms.RandomFlip(),\n",
    "            myTransforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    # segmentation masks have separate transformations (don't want to normalize)\n",
    "    seg_transformations = myTransforms.Compose([\n",
    "          myTransforms.ScaleToFixed((1, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                      interpolation=0, \n",
    "                                      channels=channels),\n",
    "            myTransforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    val_transformations = myTransforms.Compose([\n",
    "            myTransforms.MinMaxNormalize(),\n",
    "            myTransforms.ScaleToFixed((1, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                      interpolation=interpolation,\n",
    "                                      channels=channels),\n",
    "            myTransforms.ToTensor(),\n",
    "        ])\n",
    "elif channels == 4:\n",
    "    if mtl: # don't flip with MTL\n",
    "        train_transformations = myTransforms.Compose([\n",
    "                myTransforms.MinMaxNormalize(),\n",
    "                myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                          interpolation=1, \n",
    "                                          channels=channels),\n",
    "                myTransforms.ZeroSprinkle(prob_zero=0.2, prob_true=0.8),\n",
    "                myTransforms.ZeroChannel(prob_zero=0.5),\n",
    "                myTransforms.ToTensor(),\n",
    "            ])\n",
    "    else:\n",
    "            train_transformations = myTransforms.Compose([\n",
    "            myTransforms.MinMaxNormalize(),\n",
    "            myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                      interpolation=1, \n",
    "                                      channels=channels),\n",
    "            myTransforms.ZeroSprinkle(prob_zero=0.2, prob_true=0.8),\n",
    "            myTransforms.ZeroChannel(prob_zero=0.5),\n",
    "            myTransforms.RandomFlip(),\n",
    "            myTransforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    seg_transformations = myTransforms.Compose([\n",
    "        myTransforms.ScaleToFixed((1, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                      interpolation=0, \n",
    "                                      channels=1),\n",
    "            myTransforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "\n",
    "    val_transformations = myTransforms.Compose([\n",
    "            myTransforms.MinMaxNormalize(),\n",
    "            myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                      interpolation=1, \n",
    "                                      channels=channels),\n",
    "            myTransforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "\n",
    "data_transforms = {'train': train_transformations,\n",
    "                   'val':   val_transformations,\n",
    "                   'seg': seg_transformations}\n",
    "\n",
    "# train_trans, seg_trans, val_trans = get_transformations(channels=channels, resize_shape=resize_shape)\n",
    "# data_transforms = {'train': train_trans,\n",
    "#                    'val':   seg_trans,\n",
    "#                    'seg': val_trans}\n",
    "\n",
    "\n",
    "\n",
    "transformed_dataset_train = GeneralDataset(csv_file=train_df, ## change this from \"csv_file\"\n",
    "                                           root_dir=image_dir,\n",
    "                                           genomic_csv_file = genomic_data_dict['train'],\n",
    "                                           transform=data_transforms['train'],\n",
    "                                           seg_transform=data_transforms['seg'],\n",
    "                                           label=label,\n",
    "                                           classes=classes,\n",
    "                                           dataformat=dataformat,\n",
    "                                           returndims=resize_shape,\n",
    "                                           brats2tcia_df=brats2tcia_df,\n",
    "                                           null_genomic = null_genomic,\n",
    "                                           pretrained=best_model_loc,\n",
    "                                           modality=modality)\n",
    "\n",
    "transformed_dataset_val = GeneralDataset(csv_file=val_df,\n",
    "                                         root_dir=image_dir,\n",
    "                                         genomic_csv_file = genomic_data_dict['val'],\n",
    "                                         transform=data_transforms['val'],\n",
    "                                         seg_transform=data_transforms['seg'],\n",
    "                                         label=label,\n",
    "                                         classes=classes,\n",
    "                                         dataformat=dataformat,\n",
    "                                         returndims=resize_shape,\n",
    "                                         brats2tcia_df=brats2tcia_df,\n",
    "                                         null_genomic = null_genomic,\n",
    "                                         pretrained=best_model_loc,\n",
    "                                         modality=modality)\n",
    "\n",
    "\n",
    "image_datasets = {'train':transformed_dataset_train, \n",
    "                  'val':transformed_dataset_val}\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(image_datasets['train'], batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "dataloader_val = DataLoader(image_datasets['val'], batch_size=val_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val}\n",
    "\n",
    "dataset_sizes = {'train':len(image_datasets['train']), \n",
    "                 'val':len(image_datasets['val'])}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtype_dict = {0:'wildtype', 1:'val'}\n",
    "# for i, data in enumerate(dataloaders['train']):\n",
    "#     (inputs, seg_image, genomic_data, seg_probs), labels,(OS, event), bratsID = data\n",
    "#     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#     print(np.unique(seg_probs))\n",
    "    \n",
    "# #     seg_image2 = seg_probs[0].max(1)[1].data.byte().cpu().numpy()\n",
    "    \n",
    "#     print('**', inputs.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboad: 3D_idh_t1ce_mtl-True_144x144x144_genomic-True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "img_dims = str(resize_shape[0]) + 'x' + str(resize_shape[1]) + 'x' + str(resize_shape[2])\n",
    "model_outfile_dir = '3D_' + task + '_'+modality+'_mtl-' + str(mtl) + '_' + img_dims + '_genomic-' + str(null_genomic)\n",
    "print('tensorboad:', model_outfile_dir)\n",
    "writer = SummaryWriter('runs1/'+model_outfile_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 2\n",
    "# best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "\n",
    "# cluster_df = train_df['idh']\n",
    "# _, cnts = np.unique(cluster_df, return_counts=True)\n",
    "# loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts) ## \n",
    "# loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "# print('subtype class weights:', loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 2\n",
    "# best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "\n",
    "# label_df = train_df['idh']\n",
    "# _, cnts = np.unique(label_df, return_counts=True)\n",
    "# loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts)\n",
    "# loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "# criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "# print('loss weights:', loss_weights)\n",
    "\n",
    "\n",
    "# from train import train\n",
    "# best_auc_list, best_acc_list, best_auc_acc_list = [], [], []\n",
    "# epochs = 50\n",
    "# iterations = 10\n",
    "# for i in range(iterations):\n",
    "#     print('Iteration', i)\n",
    "    \n",
    "    \n",
    "#     # resize_shape = (64, 64, 64)\n",
    "\n",
    "#     from models.Models import SegModel\n",
    "    \n",
    "#     esp_model = SegModel(best_model_loc=best_model_loc, \n",
    "#                          inp_res = resize_shape, \n",
    "#                          num_classes=num_classes, \n",
    "#                          channels=4)\n",
    "    \n",
    "#     level0_weight = esp_model.espnet.level0.conv.weight[:, 0].unsqueeze(1)\n",
    "#     esp_model.espnet.level0.conv = nn.Conv3d(1, 16, kernel_size=(7, 7, 7), \n",
    "#                                              stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "\n",
    "#     esp_model.espnet.level0.conv.weight = nn.Parameter(level0_weight)\n",
    "#     esp_model = esp_model.to(device=device)\n",
    "\n",
    "#     optimizer_esp = optim.Adam(esp_model.parameters(), lr=0.0005) # change to adam\n",
    "# #     exp_scheduler = optim.lr_scheduler.StepLR(optimizer_esp, step_size=7, gamma=0.1)\n",
    "\n",
    "#     exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_esp, \n",
    "#                                                          mode='min', \n",
    "#                                                          factor=0.1, \n",
    "#                                                          patience=10, # number of epochs with no change \n",
    "#                                                          verbose=True, \n",
    "#                                                          threshold=0.0001, \n",
    "#                                                          threshold_mode='rel', \n",
    "#                                                          cooldown=0, \n",
    "#                                                          min_lr=0, \n",
    "#                                                          eps=1e-08)\n",
    "\n",
    "\n",
    "#     model, best_wts, best_auc, best_acc, best_auc_acc = train(model=esp_model, \n",
    "#                        dataloaders=dataloaders,\n",
    "#                        data_transforms=data_transforms,\n",
    "#                        criterion = criterion, \n",
    "#                        optimizer=optimizer_esp, \n",
    "#                        scheduler=exp_scheduler,\n",
    "#                        writer=writer,\n",
    "#                        num_epochs=epochs, \n",
    "#                        verbose=False, \n",
    "#                        device=device,\n",
    "#                        dataset_sizes=dataset_sizes,\n",
    "#                        channels=1,\n",
    "#                        resize_shape=resize_shape,\n",
    "#                        classes=class_names,\n",
    "#                        volume_val=False,\n",
    "#                        weight_outfile_prefix=model_outfile_dir)\n",
    "#     del esp_model\n",
    "#     del model\n",
    "    \n",
    "#     best_auc_list.append(best_auc)\n",
    "#     best_acc_list.append(best_acc)\n",
    "#     best_auc_acc_list.append(best_auc_acc)\n",
    "    \n",
    "    \n",
    "#     if not os.path.exists('../model_weights/results/'):\n",
    "#         os.makedirs('../model_weights/results/')\n",
    "    \n",
    "#     results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "#     with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "#         pickle.dump(best_auc_list, fp)\n",
    "#     with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "#         pickle.dump(best_acc_list, fp)\n",
    "#     with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "#         pickle.dump(best_auc_acc_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss weights: tensor([1.0364, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "# best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "\n",
    "label_df = train_df.loc[train_df[task].isin([0,1])][task]\n",
    "_, cnts = np.unique(label_df, return_counts=True)\n",
    "loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts)\n",
    "loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "print('loss weights:', loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc_list, best_acc_list, best_auc_acc_list = [], [], []\n",
    "epochs = 50\n",
    "iterations = 10\n",
    "    \n",
    "if not mtl:\n",
    "    print('no mtl')\n",
    "    from train import train\n",
    "    ################\n",
    "#     num_classes = 2\n",
    "#     best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "\n",
    "#     cluster_df = t_df[task + '_cluster']\n",
    "#     _, cnts = np.unique(cluster_df, return_counts=True)\n",
    "#     loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts)\n",
    "#     loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "#     print('loss weights:', loss_weights)\n",
    "\n",
    "\n",
    "#     from train import train\n",
    "#     best_auc_list, best_acc_list, best_auc_acc_list = [], [], []\n",
    "#     epochs = 50\n",
    "#     iterations = 10\n",
    "    ##############\n",
    "    \n",
    "#     label_df = labels_dict['train']\n",
    "#     label_df = label_df.loc[(label_df['phase'] == 'train') & (label_df[task] != -1)][task]\n",
    "#     # label_df = labels_dict['train'][task]\n",
    "\n",
    "#     _, cnts = np.unique(label_df, return_counts=True)\n",
    "#     loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts) ## \n",
    "#     loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "#     print('subtype class weights:', loss_weights)\n",
    "\n",
    "    if channels == 1:\n",
    "#         best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "\n",
    "#         label_df = train_df['idh']\n",
    "#         _, cnts = np.unique(label_df, return_counts=True)\n",
    "#         loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts)\n",
    "#         loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "#         criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "#         print('loss weights:', loss_weights)\n",
    "\n",
    "\n",
    "#         from train import train\n",
    "#         best_auc_list, best_acc_list, best_auc_acc_list = [], [], []\n",
    "#         epochs = 50\n",
    "#         iterations = 10\n",
    "        for i in range(iterations):\n",
    "            print('Iteration', i)\n",
    "\n",
    "\n",
    "            # resize_shape = (64, 64, 64)\n",
    "\n",
    "            from models.Models import SegModel\n",
    "\n",
    "            esp_model = SegModel(best_model_loc=best_model_loc, \n",
    "                                 inp_res = resize_shape, \n",
    "                                 num_classes=num_classes, \n",
    "                                 channels=4)\n",
    "\n",
    "            level0_weight = esp_model.espnet.level0.conv.weight[:, 0].unsqueeze(1)\n",
    "            esp_model.espnet.level0.conv = nn.Conv3d(1, 16, kernel_size=(7, 7, 7), \n",
    "                                                     stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "\n",
    "            esp_model.espnet.level0.conv.weight = nn.Parameter(level0_weight)\n",
    "            esp_model = esp_model.to(device=device)\n",
    "\n",
    "            optimizer_esp = optim.Adam(esp_model.parameters(), lr=0.0005) # change to adam\n",
    "        #     exp_scheduler = optim.lr_scheduler.StepLR(optimizer_esp, step_size=7, gamma=0.1)\n",
    "\n",
    "            exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_esp, \n",
    "                                                                 mode='min', \n",
    "                                                                 factor=0.1, \n",
    "                                                                 patience=10, # number of epochs with no change \n",
    "                                                                 verbose=True, \n",
    "                                                                 threshold=0.0001, \n",
    "                                                                 threshold_mode='rel', \n",
    "                                                                 cooldown=0, \n",
    "                                                                 min_lr=0, \n",
    "                                                                 eps=1e-08)\n",
    "\n",
    "\n",
    "            model, best_wts, best_auc, best_acc, best_auc_acc = train(model=esp_model, \n",
    "                               dataloaders=dataloaders,\n",
    "                               data_transforms=data_transforms,\n",
    "                               criterion = criterion, \n",
    "                               optimizer=optimizer_esp, \n",
    "                               scheduler=exp_scheduler,\n",
    "                               writer=writer,\n",
    "                               num_epochs=epochs, \n",
    "                               verbose=False, \n",
    "                               device=device,\n",
    "                               dataset_sizes=dataset_sizes,\n",
    "                               channels=1,\n",
    "                               resize_shape=resize_shape,\n",
    "                               classes=class_names,\n",
    "                               volume_val=False,\n",
    "                               weight_outfile_prefix=model_outfile_dir)\n",
    "            del esp_model\n",
    "            del model\n",
    "\n",
    "            best_auc_list.append(best_auc)\n",
    "            best_acc_list.append(best_acc)\n",
    "            best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "\n",
    "            if not os.path.exists('../model_weights/results/'):\n",
    "                os.makedirs('../model_weights/results/')\n",
    "\n",
    "            results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "            with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_list, fp)\n",
    "            with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_acc_list, fp)\n",
    "            with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_acc_list, fp)\n",
    "                \n",
    "    elif channels == 4:\n",
    "        criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "        for i in range(iterations):\n",
    "            print('Iteration', i)\n",
    "            from models.Models import SegModel\n",
    "            esp_model = SegModel(best_model_loc=best_model_loc, inp_res = resize_shape, num_classes=num_classes)\n",
    "\n",
    "            if channels == 1:\n",
    "                level0_weight = esp_model.espnet.level0.conv.weight[:, 0].unsqueeze(1)\n",
    "                esp_model.espnet.level0.conv = nn.Conv3d(1, 16, kernel_size=(7, 7, 7), \n",
    "                                                         stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "                esp_model.espnet.level0.conv.weight = nn.Parameter(level0_weight)\n",
    "\n",
    "\n",
    "\n",
    "            esp_model = esp_model.to(device=device)\n",
    "\n",
    "            optimizer_esp = optim.Adam(esp_model.parameters(), lr=0.0005) # change to adam\n",
    "            exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_esp, \n",
    "                                                                 mode='min', \n",
    "                                                                 factor=0.1, \n",
    "                                                                 patience=10, # number of epochs with no change \n",
    "                                                                 verbose=True, \n",
    "                                                                 threshold=0.0001, \n",
    "                                                                 threshold_mode='rel', \n",
    "                                                                 cooldown=0, \n",
    "                                                                 min_lr=0, \n",
    "                                                                 eps=1e-08)\n",
    "\n",
    "\n",
    "            \n",
    "            model, best_wts, best_auc, best_acc, best_auc_acc = train(model=esp_model, \n",
    "                               dataloaders=dataloaders,\n",
    "                               data_transforms=data_transforms,\n",
    "                               criterion = criterion, \n",
    "                               optimizer=optimizer_esp, \n",
    "                               scheduler=exp_scheduler,\n",
    "                               writer=writer,\n",
    "                               num_epochs=epochs, \n",
    "                               verbose=False, \n",
    "                               device=device,\n",
    "                               dataset_sizes=dataset_sizes,\n",
    "                               channels=1,\n",
    "                               resize_shape=resize_shape,\n",
    "                               classes=class_names,\n",
    "                               weight_outfile_prefix=model_outfile_dir)\n",
    "            del esp_model\n",
    "            del model\n",
    "\n",
    "            best_auc_list.append(best_auc)\n",
    "            best_acc_list.append(best_acc)\n",
    "            best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "\n",
    "            results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "\n",
    "            if not os.path.exists('../model_weights/results/'):\n",
    "                os.makedirs('../model_weights/results/')\n",
    "\n",
    "            with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_list, fp)\n",
    "\n",
    "            with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_acc_list, fp)\n",
    "\n",
    "            with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(best_auc_acc_list, fp)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtl\n",
      "seg_4class_weights: tensor([  1.0000, 124.3847,  60.4277, 188.0025], device='cuda:0')\n",
      "seg_2class_weights: tensor([ 1.0000, 33.4366], device='cuda:0')\n",
      "Iteration 0\n",
      "GBMNet!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training . . . \n",
      "before epochs\n",
      "  >> val_loss 0.41625828258061814 epoch 0\n",
      "  >> val AUC  0.5486111111111112 | mean acc auc 0.5243055555555556 | acc 0.5 | epoch 0\n",
      "New Best AUC-acc average:\t 0.5243055555555556 \tin epoch 0\n",
      "New Best Dice:\t 0.5014230102839139 \tin epoch 0\n",
      "New Best ACC:\t 0.5 \tin epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 1/50 [12:07<9:54:29, 727.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best AUC-acc average:\t 0.5277777777777778 \tin epoch 1\n",
      "New Best Dice:\t 0.7100452012633308 \tin epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 2/50 [24:17<9:42:51, 728.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> val_loss 0.4393914251004235 epoch 2\n",
      "  >> val AUC  0.5995370370370371 | mean acc auc 0.5497685185185186 | acc 0.5 | epoch 2\n",
      "New Best AUC-acc average:\t 0.5497685185185186 \tin epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [1:00:36<9:05:10, 726.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> val_loss 0.4352293196371046 epoch 4\n",
      "  >> val AUC  0.5393518518518519 | mean acc auc 0.5196759259259259 | acc 0.5 | epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 6/50 [1:12:42<8:52:56, 726.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> val_loss 0.40448559744883394 epoch 6\n",
      "  >> val AUC  0.681712962962963 | mean acc auc 0.6417824074074074 | acc 0.6018518518518519 | epoch 6\n",
      "New Best AUC-acc average:\t 0.6417824074074074 \tin epoch 6\n",
      "New Best AUC:\t 0.681712962962963 \tin epoch 6\n",
      "New Best ACC:\t 0.6018518518518519 \tin epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [1:49:07<8:17:15, 727.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> val_loss 0.3932077015860606 epoch 8\n",
      "  >> val AUC  0.5844907407407408 | mean acc auc 0.5422453703703705 | acc 0.5 | epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [2:13:24<7:53:17, 728.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> val_loss 0.39004761283680545 epoch 10\n",
      "  >> val AUC  0.6736111111111112 | mean acc auc 0.5868055555555556 | acc 0.5 | epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 12/50 [2:25:34<7:41:22, 728.49s/it]"
     ]
    }
   ],
   "source": [
    "if mtl:\n",
    "    print('mtl')\n",
    "    seg_loss_weight = 1\n",
    "    surv_loss_weight = 1\n",
    "    \n",
    "\n",
    "    brats_seg_ids = glioma_metadata_df[glioma_metadata_df['gt_seg'] == 1].index\n",
    "\n",
    "    seg_4class_weights = np.load('../data/segmentation_notcropped_4-class_weights.npy')\n",
    "    seg_4class_weights = torch.FloatTensor(seg_4class_weights).to(device)\n",
    "    print('seg_4class_weights:', seg_4class_weights)\n",
    "\n",
    "    seg_2class_weights = np.load('../data/segmentation_notcropped_2-class_weights.npy')\n",
    "    seg_2class_weights = torch.FloatTensor(seg_2class_weights).to(device)\n",
    "    print('seg_2class_weights:', seg_2class_weights)\n",
    "    \n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print('Iteration', i)\n",
    "\n",
    "\n",
    "        from models.nick_mtl_model import GBMNetMTL\n",
    "        gbm_net = GBMNetMTL(g_in_features=50, \n",
    "                            g_out_features=128, \n",
    "                            n_classes=num_classes, \n",
    "                            n_volumes=channels, \n",
    "                            seg_classes=4, \n",
    "                            pretrained=best_model_loc, \n",
    "                            class_loss_weights = loss_weights,\n",
    "                            seg_4class_weights=seg_4class_weights,\n",
    "                            seg_2class_weights=seg_2class_weights,\n",
    "                            seg_loss_scale=seg_loss_weight,\n",
    "                            surv_loss_scale=surv_loss_weight,\n",
    "                            device = device,\n",
    "                            brats_seg_ids=brats_seg_ids,\n",
    "                            standard_unlabled_loss=False,\n",
    "                            fusion_net_flag=True,\n",
    "                            modality=modality,\n",
    "                            take_surv_loss=False)\n",
    "        gbm_net = gbm_net.to(device)\n",
    "\n",
    "        optimizer_gbmnet = optim.Adam(gbm_net.parameters(), lr=0.0005) # change to adami\n",
    " \n",
    "        exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_gbmnet, \n",
    "                                                             mode='min', \n",
    "                                                             factor=0.1, \n",
    "                                                             patience=10, # number of epochs with no change \n",
    "                                                             verbose=True, \n",
    "                                                             threshold=0.0001, \n",
    "                                                             threshold_mode='rel', \n",
    "                                                             cooldown=0, \n",
    "                                                             min_lr=0, \n",
    "                                                             eps=1e-08)\n",
    "\n",
    "        from train_mtl import train\n",
    "        model, best_wts, best_auc, best_acc, best_auc_acc = train(model=gbm_net, \n",
    "                       dataloaders=dataloaders,\n",
    "                       data_transforms=data_transforms,\n",
    "                       optimizer=optimizer_gbmnet, \n",
    "                       scheduler=exp_scheduler,\n",
    "                       writer=writer,\n",
    "                       num_epochs=epochs, \n",
    "                       verbose=False, \n",
    "                       device=device,\n",
    "                       dataset_sizes=dataset_sizes,\n",
    "                       channels=channels,\n",
    "                       classes=class_names,\n",
    "                       weight_outfile_prefix=model_outfile_dir,\n",
    "                       pad=0)\n",
    "\n",
    "\n",
    "        del gbm_net\n",
    "        del model\n",
    "\n",
    "        best_auc_list.append(best_auc)\n",
    "        best_acc_list.append(best_acc)\n",
    "        best_auc_acc_list.append(best_auc_acc)\n",
    "\n",
    "        if not os.path.exists('../model_weights/results/'):\n",
    "            os.makedirs('../model_weights/results/')\n",
    "\n",
    "        results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "        with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(best_auc_list, fp)\n",
    "        with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(best_acc_list, fp)\n",
    "        with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(best_auc_acc_list, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to combine\n",
    "- nick_mtl_model\n",
    "- train_mtl\n",
    "- joint_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
