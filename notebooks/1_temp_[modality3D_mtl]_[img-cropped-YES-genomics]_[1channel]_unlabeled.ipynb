{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import transform # io, \n",
    "import PIL\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import pickle\n",
    "\n",
    "from utils import get_bb_3D\n",
    "\n",
    "from glob import glob\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from datasets import GeneralDataset\n",
    "import Transforms as myTransforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(glioma_metadata_df, task='idh', mtl = False):\n",
    "    '''\n",
    "    glioma_metadata_df: for each sample, indicates\n",
    "        - if it is in the labeled training, unlabeled training set, or valiation set, \n",
    "        - idh status, and 1p19q status\n",
    "    task: 'idh' or '1p19q'\n",
    "    mlt: True or False\n",
    "    '''\n",
    "    if task == 'idh':\n",
    "        classes = ['wildtype', 'mutant']\n",
    "        val_df = glioma_metadata_df.loc[(glioma_metadata_df['phase'] == 'val') & (glioma_metadata_df[task].isin([0,1]))]\n",
    "        \n",
    "\n",
    "    elif task == '1p19q':\n",
    "        classes = ['non-codel', 'oligo']\n",
    "        val_df = glioma_metadata_df.loc[glioma_metadata_df['phase'] == 'val']\n",
    "        \n",
    "    if mtl:\n",
    "        train_df = glioma_metadata_df.loc[glioma_metadata_df['phase'].isin(['train', 'unlabeled train'])]\n",
    "    else:\n",
    "        train_df = glioma_metadata_df.loc[(glioma_metadata_df['phase'] == 'train') & (glioma_metadata_df[task].isin([0,1]))]\n",
    "\n",
    "    return train_df, val_df, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl = True\n",
    "\n",
    "# task = '1p19q'\n",
    "task = 'idh'\n",
    "\n",
    "# metadata for all brats (including tcia) data\n",
    "best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "glioma_metadata_df = pd.read_csv('../data/glioma_metadata.csv', index_col=0)\n",
    "glioma_metadata_df.loc[['Brats18_TCIA09_462_1', 'Brats18_TCIA10_236_1'], 'idh'] = 1\n",
    "\n",
    "train_df, val_df, classes = get_data_splits(glioma_metadata_df=glioma_metadata_df, task=task, mtl=mtl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 467\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # we are predicting either idh gene mutation or chromosome arm 1p and 19q co-deletion\n",
    "glioma_metadata_df = pd.read_csv('../../miccai_clean/data/all_glioma_metadata_542x30.csv', index_col=0)\n",
    "glioma_metadata_df = glioma_metadata_df.rename(columns={'OS.time':'OS', '_EVENT':'OS_EVENT'})\n",
    "# if task == 'idh':\n",
    "#     classes = ['wildtype', 'mutant']\n",
    "#     val_df = glioma_metadata_df[(glioma_metadata_df['phase'] == 'val') & (glioma_metadata_df['inferred_subtype'] == 0)]\n",
    "\n",
    "# elif task == '1p19q':\n",
    "#     c\n",
    "#     print('invalid classification given')\n",
    "\n",
    "# metadata for all brats (including tcia) data\n",
    "\n",
    "brats2tcia_df = pd.read_csv('../../miccai_clean/data/brats2tcia_df_542x1.csv', index_col=0)\n",
    "\n",
    "# these are labeled files (they were paths in old dataloader) but they are dataframes\n",
    "csv_files = {'train':train_df,\n",
    "                 'val':val_df,\n",
    "                 'data':glioma_metadata_df}\n",
    "\n",
    "# genomic data (you won't need this)\n",
    "genomic_csv_files = {'train':'../data/MGL/MGL_235x50.csv', \n",
    "                     'val':  '../data/MGL/MGL_235x50.csv',\n",
    "                     'data': '../data/MGL/MGL_235x50.csv'}\n",
    "\n",
    "image_dir = '../data/all_brats_scans/'\n",
    "\n",
    "# labels we predict during classification \n",
    "cluster_column= task + '_cluster'\n",
    "\n",
    "\n",
    "# downsample dim. \n",
    "resize_shape = (64, 64, 64)\n",
    "\n",
    "# dataloader batch size\n",
    "train_batch_size = 4\n",
    "val_batch_size = 4\n",
    "data_batch_size = 1\n",
    "\n",
    "shuffle = True\n",
    "shuffle_data = True\n",
    "\n",
    "dataformat = 'modality3D_mtl'\n",
    "modality = 't1ce'\n",
    "channels = 1\n",
    "\n",
    "null_genomic = False\n",
    "\n",
    "print('Train size', len(csv_files['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wildtype', 'mutant']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minimal data augmentation (you can add more)\n",
    "train_transformations = myTransforms.Compose([\n",
    "        myTransforms.MinMaxNormalize(),\n",
    "        myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2])),\n",
    "#         myTransforms.ZeroChannel(prob_zero=0.5),\n",
    "        myTransforms.ZeroSprinkle(prob_zero=0.2, prob_true=0.8),\n",
    "        myTransforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# segmentation masks have separate transformations (you don't want to normalize)\n",
    "seg_transformations = myTransforms.Compose([\n",
    "      myTransforms.ScaleToFixed((1, resize_shape[0],resize_shape[1],resize_shape[2]), \n",
    "                                  interpolation=0, \n",
    "                                  channels=channels),\n",
    "        myTransforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "val_transformations = myTransforms.Compose([\n",
    "        myTransforms.MinMaxNormalize(),\n",
    "        myTransforms.ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2])),\n",
    "        myTransforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "data_transforms = {'train': train_transformations,\n",
    "                    'val':   val_transformations,\n",
    "                    'data':   val_transformations,\n",
    "                   'seg': seg_transformations,\n",
    "                  }\n",
    "\n",
    "\n",
    "\n",
    "best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "\n",
    "# 'jointmodel' dataformat returns entire MR images (but with the black padding cropped out)\n",
    "transformed_dataset_train = GeneralDataset(csv_file=csv_files['train'],\n",
    "                                           root_dir=image_dir,\n",
    "                                           genomic_csv_file = genomic_csv_files['train'],\n",
    "                                           transform=data_transforms['train'],\n",
    "                                           seg_transform=data_transforms['seg'],\n",
    "                                           seg_probs_transform=data_transforms['seg'],\n",
    "                                           classes=classes,\n",
    "                                           dataformat=dataformat,\n",
    "                                           returndims=resize_shape,\n",
    "                                           label=cluster_column,\n",
    "                                           brats2tcia_df=brats2tcia_df,\n",
    "                                           null_genomic = null_genomic,\n",
    "                                           pretrained=best_model_loc,\n",
    "                                           device=device,\n",
    "                                           modality=modality)\n",
    "\n",
    "transformed_dataset_val = GeneralDataset(csv_file=csv_files['val'],\n",
    "                                         root_dir=image_dir,\n",
    "                                         genomic_csv_file = genomic_csv_files['val'],\n",
    "                                         transform=data_transforms['val'],\n",
    "                                         seg_transform=data_transforms['seg'],\n",
    "                                         seg_probs_transform=data_transforms['seg'],\n",
    "                                         classes=classes,\n",
    "                                         dataformat=dataformat,\n",
    "                                         returndims=resize_shape,\n",
    "                                         label=cluster_column,\n",
    "                                         brats2tcia_df=brats2tcia_df,\n",
    "                                         null_genomic = null_genomic,\n",
    "                                         pretrained=best_model_loc,\n",
    "                                         device=device,\n",
    "                                         modality=modality)\n",
    "\n",
    "\n",
    "\n",
    "image_datasets = {'train':transformed_dataset_train, \n",
    "                  'val':transformed_dataset_val}\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(image_datasets['train'], batch_size=train_batch_size, shuffle=shuffle, num_workers=4, drop_last=True)\n",
    "dataloader_val = DataLoader(image_datasets['val'], batch_size=val_batch_size, shuffle=shuffle, num_workers=4)\n",
    "\n",
    "dataloaders = {'train':dataloader_train, 'val':dataloader_val}\n",
    "\n",
    "dataset_sizes = {'train':len(image_datasets['train']), \n",
    "                 'val':len(image_datasets['val'])}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in tqdm(enumerate(dataloaders['train'])):\n",
    "#     print('itr', i)\n",
    "#     (inputs, seg_image, genomic_data, seg_probs), labels,(OS, event), bratsID = data\n",
    "#     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#     OS, event = OS.to(device), event.to(device)\n",
    "#     seg_image, seg_probs, genomic_data = seg_image.to(device), seg_probs.to(device), genomic_data.to(device)\n",
    "#     seg_image = seg_image.squeeze(1)\n",
    "#     seg_image = seg_image.type(torch.int64)\n",
    "    \n",
    "# # #     from models import ESPNet as Net\n",
    "# # #     best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "# # #     classes = 4\n",
    "# # #     channels = 4\n",
    "\n",
    "\n",
    "# # #     espnet = Net.ESPNet(classes=classes, channels=channels)\n",
    "# # #     if os.path.isfile(best_model_loc):\n",
    "# # #         espnet.load_state_dict(torch.load(best_model_loc, map_location=device))\n",
    "\n",
    "\n",
    "# # #     level0_weight = espnet.level0.conv.weight[:, 0].unsqueeze(1)\n",
    "# # #     espnet.level0.conv = nn.Conv3d(1, 16, kernel_size=(7, 7, 7), \n",
    "# # #                                              stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "\n",
    "# # #     espnet.level0.conv.weight = nn.Parameter(level0_weight)\n",
    "\n",
    "# # #     espnet = espnet.to(device) \n",
    "\n",
    "# # #     output = espnet(inputs)\n",
    "\n",
    "# # #     seg_img = output.mask_out.max(1)[1].data.byte().cpu().numpy()\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 2\n",
    "# best_model_loc = '../pretrained/espnet_3d_brats.pth'\n",
    "\n",
    "# train_df = csv_files['train']\n",
    "# cluster_df = train_df[train_df['phase'] == 'train']['IDH'].dropna()\n",
    "\n",
    "# _, cnts = np.unique(cluster_df, return_counts=True)\n",
    "# loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts) ## \n",
    "# loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "# print('subtype class weights:', loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_df = csv_files['train']\n",
    "# label_df = label_df.loc[(label_df['phase'] == 'train') & (label_df[task] != -1)][task]\n",
    "# # label_df = labels_dict['train'][task]\n",
    "\n",
    "# _, cnts = np.unique(label_df, return_counts=True)\n",
    "# loss_weights = (np.ones(num_classes)/cnts)*np.max(cnts) ## \n",
    "# loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "# print('subtype class weights:', loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_files['train']['IDH'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtype class weights: tensor([1.0000, 1.6667], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "loss_weights = [1.0000, 1.6667]\n",
    "loss_weights = torch.FloatTensor(loss_weights).to(device)\n",
    "print('subtype class weights:', loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboad: 3D_idh_t1ce_mtl=True_64x64x64_genomic=False\n"
     ]
    }
   ],
   "source": [
    "seg_loss_weight = 1\n",
    "surv_loss_weight = 1\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "img_dims = str(resize_shape[0]) + 'x' + str(resize_shape[1]) + 'x' + str(resize_shape[2])\n",
    "model_outfile_dir = '3D_' + task + '_'+modality+'_mtl=' + str(mtl) + '_' + img_dims + '_genomic=' + str(null_genomic)\n",
    "print('tensorboad:', model_outfile_dir)\n",
    "writer = SummaryWriter('runs1/'+model_outfile_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg_4class_weights: tensor([  1.0000, 124.3847,  60.4277, 188.0025], device='cuda:0')\n",
      "seg_2class_weights: tensor([ 1.0000, 33.4366], device='cuda:0')\n",
      "Iteration 0\n",
      "GBMNet!\n",
      "Segmentation model will only use one modality (channel)\n",
      "training . . . \n",
      "before epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/app/software/SciPy-bundle/2020.03-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 4410, in get_value\n    return libindex.get_value_at(s, key)\n  File \"pandas/_libs/index.pyx\", line 44, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/index.pyx\", line 45, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/util.pxd\", line 98, in pandas._libs.util.get_value_at\n  File \"pandas/_libs/util.pxd\", line 83, in pandas._libs.util.validate_indexer\nTypeError: 'str' object cannot be interpreted as an integer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"../datasets.py\", line 161, in __getitem__\n    label = self.metadata_df.iloc[idx][self.label] # 'cluster'\n  File \"/app/software/SciPy-bundle/2020.03-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/pandas/core/series.py\", line 871, in __getitem__\n    result = self.index.get_value(self, key)\n  File \"/app/software/SciPy-bundle/2020.03-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 4418, in get_value\n    raise e1\n  File \"/app/software/SciPy-bundle/2020.03-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 4404, in get_value\n    return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n  File \"pandas/_libs/index.pyx\", line 80, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 90, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'idh_cluster'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f273fffd6839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_mtl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     model, best_wts, best_auc, best_acc, best_auc_acc = train(model=gbm_net, \n\u001b[0m\u001b[1;32m     56\u001b[0m                    \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                    \u001b[0mdata_transforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/fh/fast/holland_e/grp/HollandLabShared/Nicholas/repos/glioma_mtl/train_mtl.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, data_transforms, optimizer, scheduler, dataset_sizes, writer, num_epochs, verbose, device, channels, classes, pad, weight_dir, weight_outfile_prefix, pretrained)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mnum_samples_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenomic_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbratsID\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/app/software/SciPy-bundle/2020.03-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 4410, in get_value\n    return libindex.get_value_at(s, key)\n  File \"pandas/_libs/index.pyx\", line 44, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/index.pyx\", line 45, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/util.pxd\", line 98, in pandas._libs.util.get_value_at\n  File \"pandas/_libs/util.pxd\", line 83, in pandas._libs.util.validate_indexer\nTypeError: 'str' object cannot be interpreted as an integer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/app/software/fhPython/3.8.2-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"../datasets.py\", line 161, in __getitem__\n    label = self.metadata_df.iloc[idx][self.label] # 'cluster'\n  File \"/app/software/SciPy-bundle/2020.03-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/pandas/core/series.py\", line 871, in __getitem__\n    result = self.index.get_value(self, key)\n  File \"/app/software/SciPy-bundle/2020.03-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 4418, in get_value\n    raise e1\n  File \"/app/software/SciPy-bundle/2020.03-foss-2020a-Python-3.8.2/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 4404, in get_value\n    return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n  File \"pandas/_libs/index.pyx\", line 80, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 90, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'idh_cluster'\n"
     ]
    }
   ],
   "source": [
    "best_auc_list, best_acc_list, best_auc_acc_list = [], [], []\n",
    "epochs = 20\n",
    "iterations = 10\n",
    "\n",
    "brats_seg_ids = glioma_metadata_df[glioma_metadata_df['gt_seg'] == 1].index\n",
    "\n",
    "seg_4class_weights = np.load('../data/segmentation_notcropped_4-class_weights.npy')\n",
    "seg_4class_weights = torch.FloatTensor(seg_4class_weights).to(device)\n",
    "print('seg_4class_weights:', seg_4class_weights)\n",
    "\n",
    "seg_2class_weights = np.load('../data/segmentation_notcropped_2-class_weights.npy')\n",
    "seg_2class_weights = torch.FloatTensor(seg_2class_weights).to(device)\n",
    "print('seg_2class_weights:', seg_2class_weights)\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Iteration', i)\n",
    "    \n",
    "\n",
    "    from models.nick_mtl_model import GBMNetMTL\n",
    "    gbm_net = GBMNetMTL(g_in_features=50, \n",
    "                        g_out_features=128, \n",
    "                        n_classes=num_classes, \n",
    "                        n_volumes=1, \n",
    "                        seg_classes=4, \n",
    "                        pretrained=best_model_loc, \n",
    "                        class_loss_weights = loss_weights,\n",
    "                        seg_4class_weights=seg_4class_weights,\n",
    "                        seg_2class_weights=seg_2class_weights,\n",
    "                        seg_loss_scale=seg_loss_weight,\n",
    "                        surv_loss_scale=surv_loss_weight,\n",
    "                        device = device,\n",
    "                        brats_seg_ids=brats_seg_ids,\n",
    "                        standard_unlabled_loss=False,\n",
    "                        fusion_net_flag=True,\n",
    "                        modality=modality,\n",
    "                        take_surv_loss=False) # might have to put loss_weights on device.\n",
    "\n",
    "    gbm_net = gbm_net.to(device)\n",
    "\n",
    "    optimizer_gbmnet = optim.Adam(gbm_net.parameters(), lr=0.0005) # change to adami\n",
    "#     exp_scheduler = optim.lr_scheduler.StepLR(optimizer_gbmnet, step_size=7, gamma=0.1)\n",
    "    \n",
    "    exp_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_gbmnet, \n",
    "                                                         mode='min', \n",
    "                                                         factor=0.1, \n",
    "                                                         patience=10, # number of epochs with no change \n",
    "                                                         verbose=True, \n",
    "                                                         threshold=0.0001, \n",
    "                                                         threshold_mode='rel', \n",
    "                                                         cooldown=0, \n",
    "                                                         min_lr=0, \n",
    "                                                         eps=1e-08)\n",
    "\n",
    "    from train_mtl import train\n",
    "    model, best_wts, best_auc, best_acc, best_auc_acc = train(model=gbm_net, \n",
    "                   dataloaders=dataloaders,\n",
    "                   data_transforms=data_transforms,\n",
    "                   optimizer=optimizer_gbmnet, \n",
    "                   scheduler=exp_scheduler,\n",
    "                   writer=writer,\n",
    "                   num_epochs=epochs, \n",
    "                   verbose=False, \n",
    "                   device=device,\n",
    "                   dataset_sizes=dataset_sizes,\n",
    "                   channels=1,\n",
    "                   classes=class_names,\n",
    "                   weight_outfile_prefix=model_outfile_dir,\n",
    "                   pad=0)\n",
    "\n",
    "    \n",
    "    del gbm_net\n",
    "    del model\n",
    "    \n",
    "    best_auc_list.append(best_auc)\n",
    "    best_acc_list.append(best_acc)\n",
    "    best_auc_acc_list.append(best_auc_acc)\n",
    "    \n",
    "    if not os.path.exists('../model_weights/results/'):\n",
    "        os.makedirs('../model_weights/results/')\n",
    "    \n",
    "    results_outfile_dir = model_outfile_dir + '_epochs-' + str(epochs) +'_iterations-' + str(iterations)\n",
    "    with open('../model_weights/results/auc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(best_auc_list, fp)\n",
    "    with open('../model_weights/results/acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(best_acc_list, fp)\n",
    "    with open('../model_weights/results/avg_auc_acc_' + results_outfile_dir + '.txt', \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(best_auc_acc_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(best_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "T1ce: 50 epochs, 10 itr (This was MTL without the surival loss). This is the number we wanted the highest)\n",
    "\n",
    "3D_idh_t1ce_mtl_64x64x64_null-genomic_seglossweight-1_zero-sprinkle-channel\n",
    "\n",
    "    - auc mean: 0.8840277777777779\n",
    "    - auc std: 0.015952080698166733\n",
    "\n",
    "    - acc mean: 0.779050925925926\n",
    "    - acc std: 0.05753653711830886\n",
    "---\n",
    "\n",
    "3D_1p19q_t1ce_mtl_64x64x64_segLoss1_survLoss1\n",
    "\n",
    "    - auc mean: 0.8054545454545454\n",
    "    - auc std: 0.025454545454545462\n",
    "\n",
    "    - acc mean: 0.5198484848484848\n",
    "    - acc std: 0.030906491275110404\n",
    "    \n",
    "3D_1p19q_t1ce-t1_mtl_64x64x64_segLoss1_survLoss1\n",
    "\n",
    "    - auc mean: 0.7426666666666668\n",
    "    - auc std: 0.05036214529795515\n",
    "\n",
    "    - acc mean: 0.5\n",
    "    - acc std: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(best_auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
